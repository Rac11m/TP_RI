{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DropZone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Index/DescripteursSplitLancaster.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../Index/DescripteursSplitLancaster.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f :\n\u001b[0;32m      2\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Index/DescripteursSplitPorter.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f :\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Index/DescripteursSplitLancaster.txt'"
     ]
    }
   ],
   "source": [
    "with open(file=\"../Index/DescripteursSplitLancaster.txt\", mode='w') as f :\n",
    "    f.write('')\n",
    "with open(file=\"../Index/DescripteursSplitPorter.txt\", mode='w') as f :\n",
    "    f.write('')\n",
    "with open(file=\"../Index/DescripteursTokenLancaster.txt\", mode='w') as f :\n",
    "    f.write('')\n",
    "with open(file=\"../Index/DescripteursTokenPorter.txt\", mode='w') as f :\n",
    "    f.write('')\n",
    "with open(file=\"../Index/InverseSplitLancaster.txt\", mode='w') as f :\n",
    "    f.write('')\n",
    "with open(file=\"../Index/InverseSplitPorter.txt\", mode='w') as f :\n",
    "    f.write('')\n",
    "with open(file=\"../Index/InverseTokenLancaster.txt\", mode='w') as f :\n",
    "    f.write('')\n",
    "with open(file=\"../Index/InverseTokenPorter.txt\", mode='w') as f :\n",
    "    f.write('')\n",
    "\n",
    "# with open(file='./*.txt', mode='r') as f:\n",
    "#     txt = f.read()\n",
    "# print(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D1': 'Query Reformulation (QR) is a set of techniques used to transform a user’s original search query to a text that better aligns with the user’s intent and improves their search experience. Recently, zero-shot QR has been shown to be a promising approach due to its ability to exploit knowledge inherent in large language models. By taking inspiration from the success of ensemble prompting strategies which have benefited many tasks, we investigate if they can help improve query reformulation. In this context, we propose an ensemble based prompting technique, GenQREnsemble which leverages paraphrases of a zero-shot instruction to generate multiple sets of keywords ultimately improving retrieval performance. We further introduce its post-retrieval variant, GenQREnsembleRF to incorporate pseudo relevant feedback. On evaluations over four IR benchmarks, we find that GenQREnsemble generates better reformulations with relative nDCG@10 improvements up to 18% and MAP improvements upto 24% over the previous zero-shot state-of-art. On the MSMarco Passage Ranking task, GenQREnsembleRF shows relative gains of 5% MRR using pseudo-relevance feedback, and 9% nDCG@10 using relevant feedback documents.', 'D2': 'Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these challenging ranking formulations. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019 and 2020, PRP based on the Flan-UL2 model with 20B parameters performs favorably with the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that has 50x (estimated) model size, while outperforming other LLM-based solutions, such as InstructGPT which has 175B parameters, by over 12-10% for all ranking metrics. By using the same prompt template on seven BEIR tasks, PRP outperforms supervised baselines and outperforms the blackbox commercial ChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on average NDCG@10. Furthermore, we propose several variants of PRP to improve efficiency and show that it is possible to achieve competitive results even with linear complexity', 'D3': 'Large language models (LLM) have manifested unparalleled modeling capability on various tasks, e.g., multi-step reasoning, but the input to these models is mostly limited to plain text, which could be very long and contain noisy information. Long text could take long time to process, and thus may not be efficient enough for recommender systems that require immediate response. In LLM-based recommendation models, user and item IDs are usually filled in a template (i.e., discrete prompt) to allow the models to understand a given task, but the models usually need extensive fine-tuning to bridge the user/item IDs and the template words and to unleash the power of LLM for recommendation. To address the problems, we propose to distill the discrete prompt for a specific task to a set of continuous prompt vectors so as to bridge IDs and words and to reduce the inference time. We also design a training strategy with an attempt to improve the efficiency of training these models. Experimental results on three real-world datasets demonstrate the effectiveness of our PrOmpt Distillation (POD) approach on both sequential recommendation and top-N recommendation tasks. Although the training efficiency can be significantly improved, the improvement of inference efficiency is limited. This finding may inspire researchers in the community to further improve the inference efficiency of LLM-based recommendation models.', 'D4': \"Query reformulation is a well-known problem in Information Retrieval (IR) aimed at enhancing single search successful completion rate by automatically modifying user's input query. Recent methods leverage Large Language Models (LLMs) to improve query reformulation, but often generate limited and redundant expansions, potentially constraining their effectiveness in capturing diverse intents. In this paper, we propose GenCRF: a Generative Clustering and Reformulation Framework to capture diverse intentions adaptively based on multiple differentiated, well-generated queries in the retrieval phase for the first time. GenCRF leverages LLMs to generate variable queries from the initial query using customized prompts, then clusters them into groups to distinctly represent diverse intents. Furthermore, the framework explores to combine diverse intents query with innovative weighted aggregation strategies to optimize retrieval performance and crucially integrates a novel Query Evaluation Rewarding Model (QERM) to refine the process through feedback loops. Empirical experiments on the BEIR benchmark demonstrate that GenCRF achieves state-of-the-art performance, surpassing previous query reformulation SOTAs by up to 12% on nDCG@10. These techniques can be adapted to various LLMs, significantly boosting retriever performance and advancing the field of Information Retrieval.\", 'D5': \"Session search involves a series of interactive queries and actions to fulfill user's complex information need. Current strategies typically prioritize sequential modeling for deep semantic understanding, overlooking the graph structure in interactions. While some approaches focus on capturing structural information, they use a generalized representation for documents, neglecting the word-level semantic modeling. In this paper, we propose Symbolic Graph Ranker (SGR), which aims to take advantage of both text-based and graph-based approaches by leveraging the power of recent Large Language Models (LLMs). Concretely, we first introduce a set of symbolic grammar rules to convert session graph into text. This allows integrating session history, interaction process, and task instruction seamlessly as inputs for the LLM. Moreover, given the natural discrepancy between LLMs pre-trained on textual corpora, and the symbolic language we produce using our graph-to-text grammar, our objective is to enhance LLMs' ability to capture graph structures within a textual format. To achieve this, we introduce a set of self-supervised symbolic learning tasks including link prediction, node content generation, and generative contrastive learning, to enable LLMs to capture the topological information from coarse-grained to fine-grained. Experiment results and comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm the superiority of our approach. Our paradigm also offers a novel and effective methodology that bridges the gap between traditional search strategies and modern LLMs.\", 'D6': 'The description of an item plays a pivotal role in providing concise and informative summaries to captivate potential viewers and is essential for recommendation systems. Traditionally, such descriptions were obtained through manual web scraping techniques, which are time-consuming and susceptible to data inconsistencies. In recent years, Large Language Models (LLMs), such as GPT-3.5, and open source LLMs like Alpaca have emerged as powerful tools for natural language processing tasks. In this paper, we have explored how we can use LLMs to generate detailed descriptions of the items. To conduct the study, we have used the MovieLens 1M dataset comprising movie titles and the Goodreads Dataset consisting of names of books and subsequently, an open-sourced LLM, Alpaca, was prompted with few-shot prompting on this dataset to generate detailed movie descriptions considering multiple features like the names of the cast and directors for the ML dataset and the names of the author and publisher for the Goodreads dataset. The generated description was then compared with the scraped descriptions using a combination of Top Hits, MRR, and NDCG as evaluation metrics. The results demonstrated that LLM-based movie description generation exhibits significant promise, with results comparable to the ones obtained by web-scraped descriptions.'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = '../Collection/'\n",
    "\n",
    "# print(os.path.exists(path))\n",
    "\n",
    "os.chdir(path)\n",
    "\n",
    "def read_txt_file(file_path):\n",
    "    with open(file=file_path, mode='r') as f:\n",
    "        # print(f.read())\n",
    "        return f.read()\n",
    "    \n",
    "txt_contents = {}\n",
    "\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file.endswith('.txt'):\n",
    "        file_path = f'{path}/{file}'\n",
    "\n",
    "        file_name = os.path.splitext(file)[0]\n",
    "        txt_contents[file_name] = read_txt_file(file_path)\n",
    "\n",
    "print(txt_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer, LancasterStemmer\n",
    "\n",
    "Porter = PorterStemmer()\n",
    "Lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D1': ['query', 'reformulation', '(qr)', 'is', 'a', 'set', 'of', 'techniques', 'used', 'to', 'transform', 'a', 'user’s', 'original', 'search', 'query', 'to', 'a', 'text', 'that', 'better', 'aligns', 'with', 'the', 'user’s', 'intent', 'and', 'improves', 'their', 'search', 'experience.', 'recently,', 'zero-shot', 'qr', 'has', 'been', 'shown', 'to', 'be', 'a', 'promising', 'approach', 'due', 'to', 'its', 'ability', 'to', 'exploit', 'knowledge', 'inherent', 'in', 'large', 'language', 'models.', 'by', 'taking', 'inspiration', 'from', 'the', 'success', 'of', 'ensemble', 'prompting', 'strategies', 'which', 'have', 'benefited', 'many', 'tasks,', 'we', 'investigate', 'if', 'they', 'can', 'help', 'improve', 'query', 'reformulation.', 'in', 'this', 'context,', 'we', 'propose', 'an', 'ensemble', 'based', 'prompting', 'technique,', 'genqrensemble', 'which', 'leverages', 'paraphrases', 'of', 'a', 'zero-shot', 'instruction', 'to', 'generate', 'multiple', 'sets', 'of', 'keywords', 'ultimately', 'improving', 'retrieval', 'performance.', 'we', 'further', 'introduce', 'its', 'post-retrieval', 'variant,', 'genqrensemblerf', 'to', 'incorporate', 'pseudo', 'relevant', 'feedback.', 'on', 'evaluations', 'over', 'four', 'ir', 'benchmarks,', 'we', 'find', 'that', 'genqrensemble', 'generates', 'better', 'reformulations', 'with', 'relative', 'ndcg@10', 'improvements', 'up', 'to', '18%', 'and', 'map', 'improvements', 'upto', '24%', 'over', 'the', 'previous', 'zero-shot', 'state-of-art.', 'on', 'the', 'msmarco', 'passage', 'ranking', 'task,', 'genqrensemblerf', 'shows', 'relative', 'gains', 'of', '5%', 'mrr', 'using', 'pseudo-relevance', 'feedback,', 'and', '9%', 'ndcg@10', 'using', 'relevant', 'feedback', 'documents.'], 'D2': ['ranking', 'documents', 'using', 'large', 'language', 'models', '(llms)', 'by', 'directly', 'feeding', 'the', 'query', 'and', 'candidate', 'documents', 'into', 'the', 'prompt', 'is', 'an', 'interesting', 'and', 'practical', 'problem.', 'however,', 'researchers', 'have', 'found', 'it', 'difficult', 'to', 'outperform', 'fine-tuned', 'baseline', 'rankers', 'on', 'benchmark', 'datasets.', 'we', 'analyze', 'pointwise', 'and', 'listwise', 'ranking', 'prompts', 'used', 'by', 'existing', 'methods', 'and', 'argue', 'that', 'off-the-shelf', 'llms', 'do', 'not', 'fully', 'understand', 'these', 'challenging', 'ranking', 'formulations.', 'in', 'this', 'paper,', 'we', 'propose', 'to', 'significantly', 'reduce', 'the', 'burden', 'on', 'llms', 'by', 'using', 'a', 'new', 'technique', 'called', 'pairwise', 'ranking', 'prompting', '(prp).', 'our', 'results', 'are', 'the', 'first', 'in', 'the', 'literature', 'to', 'achieve', 'state-of-the-art', 'ranking', 'performance', 'on', 'standard', 'benchmarks', 'using', 'moderate-sized', 'open-sourced', 'llms.', 'on', 'trec-dl', '2019', 'and', '2020,', 'prp', 'based', 'on', 'the', 'flan-ul2', 'model', 'with', '20b', 'parameters', 'performs', 'favorably', 'with', 'the', 'previous', 'best', 'approach', 'in', 'the', 'literature,', 'which', 'is', 'based', 'on', 'the', 'blackbox', 'commercial', 'gpt-4', 'that', 'has', '50x', '(estimated)', 'model', 'size,', 'while', 'outperforming', 'other', 'llm-based', 'solutions,', 'such', 'as', 'instructgpt', 'which', 'has', '175b', 'parameters,', 'by', 'over', '12-10%', 'for', 'all', 'ranking', 'metrics.', 'by', 'using', 'the', 'same', 'prompt', 'template', 'on', 'seven', 'beir', 'tasks,', 'prp', 'outperforms', 'supervised', 'baselines', 'and', 'outperforms', 'the', 'blackbox', 'commercial', 'chatgpt', 'solution', 'by', '4.2%', 'and', 'pointwise', 'llm-based', 'solutions', 'by', 'more', 'than', '10%', 'on', 'average', 'ndcg@10.', 'furthermore,', 'we', 'propose', 'several', 'variants', 'of', 'prp', 'to', 'improve', 'efficiency', 'and', 'show', 'that', 'it', 'is', 'possible', 'to', 'achieve', 'competitive', 'results', 'even', 'with', 'linear', 'complexity'], 'D3': ['large', 'language', 'models', '(llm)', 'have', 'manifested', 'unparalleled', 'modeling', 'capability', 'on', 'various', 'tasks,', 'e.g.,', 'multi-step', 'reasoning,', 'but', 'the', 'input', 'to', 'these', 'models', 'is', 'mostly', 'limited', 'to', 'plain', 'text,', 'which', 'could', 'be', 'very', 'long', 'and', 'contain', 'noisy', 'information.', 'long', 'text', 'could', 'take', 'long', 'time', 'to', 'process,', 'and', 'thus', 'may', 'not', 'be', 'efficient', 'enough', 'for', 'recommender', 'systems', 'that', 'require', 'immediate', 'response.', 'in', 'llm-based', 'recommendation', 'models,', 'user', 'and', 'item', 'ids', 'are', 'usually', 'filled', 'in', 'a', 'template', '(i.e.,', 'discrete', 'prompt)', 'to', 'allow', 'the', 'models', 'to', 'understand', 'a', 'given', 'task,', 'but', 'the', 'models', 'usually', 'need', 'extensive', 'fine-tuning', 'to', 'bridge', 'the', 'user/item', 'ids', 'and', 'the', 'template', 'words', 'and', 'to', 'unleash', 'the', 'power', 'of', 'llm', 'for', 'recommendation.', 'to', 'address', 'the', 'problems,', 'we', 'propose', 'to', 'distill', 'the', 'discrete', 'prompt', 'for', 'a', 'specific', 'task', 'to', 'a', 'set', 'of', 'continuous', 'prompt', 'vectors', 'so', 'as', 'to', 'bridge', 'ids', 'and', 'words', 'and', 'to', 'reduce', 'the', 'inference', 'time.', 'we', 'also', 'design', 'a', 'training', 'strategy', 'with', 'an', 'attempt', 'to', 'improve', 'the', 'efficiency', 'of', 'training', 'these', 'models.', 'experimental', 'results', 'on', 'three', 'real-world', 'datasets', 'demonstrate', 'the', 'effectiveness', 'of', 'our', 'prompt', 'distillation', '(pod)', 'approach', 'on', 'both', 'sequential', 'recommendation', 'and', 'top-n', 'recommendation', 'tasks.', 'although', 'the', 'training', 'efficiency', 'can', 'be', 'significantly', 'improved,', 'the', 'improvement', 'of', 'inference', 'efficiency', 'is', 'limited.', 'this', 'finding', 'may', 'inspire', 'researchers', 'in', 'the', 'community', 'to', 'further', 'improve', 'the', 'inference', 'efficiency', 'of', 'llm-based', 'recommendation', 'models.'], 'D4': ['query', 'reformulation', 'is', 'a', 'well-known', 'problem', 'in', 'information', 'retrieval', '(ir)', 'aimed', 'at', 'enhancing', 'single', 'search', 'successful', 'completion', 'rate', 'by', 'automatically', 'modifying', \"user's\", 'input', 'query.', 'recent', 'methods', 'leverage', 'large', 'language', 'models', '(llms)', 'to', 'improve', 'query', 'reformulation,', 'but', 'often', 'generate', 'limited', 'and', 'redundant', 'expansions,', 'potentially', 'constraining', 'their', 'effectiveness', 'in', 'capturing', 'diverse', 'intents.', 'in', 'this', 'paper,', 'we', 'propose', 'gencrf:', 'a', 'generative', 'clustering', 'and', 'reformulation', 'framework', 'to', 'capture', 'diverse', 'intentions', 'adaptively', 'based', 'on', 'multiple', 'differentiated,', 'well-generated', 'queries', 'in', 'the', 'retrieval', 'phase', 'for', 'the', 'first', 'time.', 'gencrf', 'leverages', 'llms', 'to', 'generate', 'variable', 'queries', 'from', 'the', 'initial', 'query', 'using', 'customized', 'prompts,', 'then', 'clusters', 'them', 'into', 'groups', 'to', 'distinctly', 'represent', 'diverse', 'intents.', 'furthermore,', 'the', 'framework', 'explores', 'to', 'combine', 'diverse', 'intents', 'query', 'with', 'innovative', 'weighted', 'aggregation', 'strategies', 'to', 'optimize', 'retrieval', 'performance', 'and', 'crucially', 'integrates', 'a', 'novel', 'query', 'evaluation', 'rewarding', 'model', '(qerm)', 'to', 'refine', 'the', 'process', 'through', 'feedback', 'loops.', 'empirical', 'experiments', 'on', 'the', 'beir', 'benchmark', 'demonstrate', 'that', 'gencrf', 'achieves', 'state-of-the-art', 'performance,', 'surpassing', 'previous', 'query', 'reformulation', 'sotas', 'by', 'up', 'to', '12%', 'on', 'ndcg@10.', 'these', 'techniques', 'can', 'be', 'adapted', 'to', 'various', 'llms,', 'significantly', 'boosting', 'retriever', 'performance', 'and', 'advancing', 'the', 'field', 'of', 'information', 'retrieval.'], 'D5': ['session', 'search', 'involves', 'a', 'series', 'of', 'interactive', 'queries', 'and', 'actions', 'to', 'fulfill', \"user's\", 'complex', 'information', 'need.', 'current', 'strategies', 'typically', 'prioritize', 'sequential', 'modeling', 'for', 'deep', 'semantic', 'understanding,', 'overlooking', 'the', 'graph', 'structure', 'in', 'interactions.', 'while', 'some', 'approaches', 'focus', 'on', 'capturing', 'structural', 'information,', 'they', 'use', 'a', 'generalized', 'representation', 'for', 'documents,', 'neglecting', 'the', 'word-level', 'semantic', 'modeling.', 'in', 'this', 'paper,', 'we', 'propose', 'symbolic', 'graph', 'ranker', '(sgr),', 'which', 'aims', 'to', 'take', 'advantage', 'of', 'both', 'text-based', 'and', 'graph-based', 'approaches', 'by', 'leveraging', 'the', 'power', 'of', 'recent', 'large', 'language', 'models', '(llms).', 'concretely,', 'we', 'first', 'introduce', 'a', 'set', 'of', 'symbolic', 'grammar', 'rules', 'to', 'convert', 'session', 'graph', 'into', 'text.', 'this', 'allows', 'integrating', 'session', 'history,', 'interaction', 'process,', 'and', 'task', 'instruction', 'seamlessly', 'as', 'inputs', 'for', 'the', 'llm.', 'moreover,', 'given', 'the', 'natural', 'discrepancy', 'between', 'llms', 'pre-trained', 'on', 'textual', 'corpora,', 'and', 'the', 'symbolic', 'language', 'we', 'produce', 'using', 'our', 'graph-to-text', 'grammar,', 'our', 'objective', 'is', 'to', 'enhance', \"llms'\", 'ability', 'to', 'capture', 'graph', 'structures', 'within', 'a', 'textual', 'format.', 'to', 'achieve', 'this,', 'we', 'introduce', 'a', 'set', 'of', 'self-supervised', 'symbolic', 'learning', 'tasks', 'including', 'link', 'prediction,', 'node', 'content', 'generation,', 'and', 'generative', 'contrastive', 'learning,', 'to', 'enable', 'llms', 'to', 'capture', 'the', 'topological', 'information', 'from', 'coarse-grained', 'to', 'fine-grained.', 'experiment', 'results', 'and', 'comprehensive', 'analysis', 'on', 'two', 'benchmark', 'datasets,', 'aol', 'and', 'tiangong-st,', 'confirm', 'the', 'superiority', 'of', 'our', 'approach.', 'our', 'paradigm', 'also', 'offers', 'a', 'novel', 'and', 'effective', 'methodology', 'that', 'bridges', 'the', 'gap', 'between', 'traditional', 'search', 'strategies', 'and', 'modern', 'llms.'], 'D6': ['the', 'description', 'of', 'an', 'item', 'plays', 'a', 'pivotal', 'role', 'in', 'providing', 'concise', 'and', 'informative', 'summaries', 'to', 'captivate', 'potential', 'viewers', 'and', 'is', 'essential', 'for', 'recommendation', 'systems.', 'traditionally,', 'such', 'descriptions', 'were', 'obtained', 'through', 'manual', 'web', 'scraping', 'techniques,', 'which', 'are', 'time-consuming', 'and', 'susceptible', 'to', 'data', 'inconsistencies.', 'in', 'recent', 'years,', 'large', 'language', 'models', '(llms),', 'such', 'as', 'gpt-3.5,', 'and', 'open', 'source', 'llms', 'like', 'alpaca', 'have', 'emerged', 'as', 'powerful', 'tools', 'for', 'natural', 'language', 'processing', 'tasks.', 'in', 'this', 'paper,', 'we', 'have', 'explored', 'how', 'we', 'can', 'use', 'llms', 'to', 'generate', 'detailed', 'descriptions', 'of', 'the', 'items.', 'to', 'conduct', 'the', 'study,', 'we', 'have', 'used', 'the', 'movielens', '1m', 'dataset', 'comprising', 'movie', 'titles', 'and', 'the', 'goodreads', 'dataset', 'consisting', 'of', 'names', 'of', 'books', 'and', 'subsequently,', 'an', 'open-sourced', 'llm,', 'alpaca,', 'was', 'prompted', 'with', 'few-shot', 'prompting', 'on', 'this', 'dataset', 'to', 'generate', 'detailed', 'movie', 'descriptions', 'considering', 'multiple', 'features', 'like', 'the', 'names', 'of', 'the', 'cast', 'and', 'directors', 'for', 'the', 'ml', 'dataset', 'and', 'the', 'names', 'of', 'the', 'author', 'and', 'publisher', 'for', 'the', 'goodreads', 'dataset.', 'the', 'generated', 'description', 'was', 'then', 'compared', 'with', 'the', 'scraped', 'descriptions', 'using', 'a', 'combination', 'of', 'top', 'hits,', 'mrr,', 'and', 'ndcg', 'as', 'evaluation', 'metrics.', 'the', 'results', 'demonstrated', 'that', 'llm-based', 'movie', 'description', 'generation', 'exhibits', 'significant', 'promise,', 'with', 'results', 'comparable', 'to', 'the', 'ones', 'obtained', 'by', 'web-scraped', 'descriptions.']}\n"
     ]
    }
   ],
   "source": [
    "termes = {file_name: txt.lower().split() for file_name, txt in txt_contents.items()}\n",
    "print(termes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D1': ['query', 'reformulation', '(qr)', 'set', 'techniques', 'used', 'transform', 'user’s', 'original', 'search', 'query', 'text', 'better', 'aligns', 'user’s', 'intent', 'improves', 'search', 'experience.', 'recently,', 'zero-shot', 'qr', 'shown', 'promising', 'approach', 'due', 'ability', 'exploit', 'knowledge', 'inherent', 'large', 'language', 'models.', 'taking', 'inspiration', 'success', 'ensemble', 'prompting', 'strategies', 'benefited', 'many', 'tasks,', 'investigate', 'help', 'improve', 'query', 'reformulation.', 'context,', 'propose', 'ensemble', 'based', 'prompting', 'technique,', 'genqrensemble', 'leverages', 'paraphrases', 'zero-shot', 'instruction', 'generate', 'multiple', 'sets', 'keywords', 'ultimately', 'improving', 'retrieval', 'performance.', 'introduce', 'post-retrieval', 'variant,', 'genqrensemblerf', 'incorporate', 'pseudo', 'relevant', 'feedback.', 'evaluations', 'four', 'ir', 'benchmarks,', 'find', 'genqrensemble', 'generates', 'better', 'reformulations', 'relative', 'ndcg@10', 'improvements', '18%', 'map', 'improvements', 'upto', '24%', 'previous', 'zero-shot', 'state-of-art.', 'msmarco', 'passage', 'ranking', 'task,', 'genqrensemblerf', 'shows', 'relative', 'gains', '5%', 'mrr', 'using', 'pseudo-relevance', 'feedback,', '9%', 'ndcg@10', 'using', 'relevant', 'feedback', 'documents.'], 'D2': ['ranking', 'documents', 'using', 'large', 'language', 'models', '(llms)', 'directly', 'feeding', 'query', 'candidate', 'documents', 'prompt', 'interesting', 'practical', 'problem.', 'however,', 'researchers', 'found', 'difficult', 'outperform', 'fine-tuned', 'baseline', 'rankers', 'benchmark', 'datasets.', 'analyze', 'pointwise', 'listwise', 'ranking', 'prompts', 'used', 'existing', 'methods', 'argue', 'off-the-shelf', 'llms', 'fully', 'understand', 'challenging', 'ranking', 'formulations.', 'paper,', 'propose', 'significantly', 'reduce', 'burden', 'llms', 'using', 'new', 'technique', 'called', 'pairwise', 'ranking', 'prompting', '(prp).', 'results', 'first', 'literature', 'achieve', 'state-of-the-art', 'ranking', 'performance', 'standard', 'benchmarks', 'using', 'moderate-sized', 'open-sourced', 'llms.', 'trec-dl', '2019', '2020,', 'prp', 'based', 'flan-ul2', 'model', '20b', 'parameters', 'performs', 'favorably', 'previous', 'best', 'approach', 'literature,', 'based', 'blackbox', 'commercial', 'gpt-4', '50x', '(estimated)', 'model', 'size,', 'outperforming', 'llm-based', 'solutions,', 'instructgpt', '175b', 'parameters,', '12-10%', 'ranking', 'metrics.', 'using', 'prompt', 'template', 'seven', 'beir', 'tasks,', 'prp', 'outperforms', 'supervised', 'baselines', 'outperforms', 'blackbox', 'commercial', 'chatgpt', 'solution', '4.2%', 'pointwise', 'llm-based', 'solutions', '10%', 'average', 'ndcg@10.', 'furthermore,', 'propose', 'several', 'variants', 'prp', 'improve', 'efficiency', 'show', 'possible', 'achieve', 'competitive', 'results', 'even', 'linear', 'complexity'], 'D3': ['large', 'language', 'models', '(llm)', 'manifested', 'unparalleled', 'modeling', 'capability', 'various', 'tasks,', 'e.g.,', 'multi-step', 'reasoning,', 'input', 'models', 'mostly', 'limited', 'plain', 'text,', 'could', 'long', 'contain', 'noisy', 'information.', 'long', 'text', 'could', 'take', 'long', 'time', 'process,', 'thus', 'may', 'efficient', 'enough', 'recommender', 'systems', 'require', 'immediate', 'response.', 'llm-based', 'recommendation', 'models,', 'user', 'item', 'ids', 'usually', 'filled', 'template', '(i.e.,', 'discrete', 'prompt)', 'allow', 'models', 'understand', 'given', 'task,', 'models', 'usually', 'need', 'extensive', 'fine-tuning', 'bridge', 'user/item', 'ids', 'template', 'words', 'unleash', 'power', 'llm', 'recommendation.', 'address', 'problems,', 'propose', 'distill', 'discrete', 'prompt', 'specific', 'task', 'set', 'continuous', 'prompt', 'vectors', 'bridge', 'ids', 'words', 'reduce', 'inference', 'time.', 'also', 'design', 'training', 'strategy', 'attempt', 'improve', 'efficiency', 'training', 'models.', 'experimental', 'results', 'three', 'real-world', 'datasets', 'demonstrate', 'effectiveness', 'prompt', 'distillation', '(pod)', 'approach', 'sequential', 'recommendation', 'top-n', 'recommendation', 'tasks.', 'although', 'training', 'efficiency', 'significantly', 'improved,', 'improvement', 'inference', 'efficiency', 'limited.', 'finding', 'may', 'inspire', 'researchers', 'community', 'improve', 'inference', 'efficiency', 'llm-based', 'recommendation', 'models.'], 'D4': ['query', 'reformulation', 'well-known', 'problem', 'information', 'retrieval', '(ir)', 'aimed', 'enhancing', 'single', 'search', 'successful', 'completion', 'rate', 'automatically', 'modifying', \"user's\", 'input', 'query.', 'recent', 'methods', 'leverage', 'large', 'language', 'models', '(llms)', 'improve', 'query', 'reformulation,', 'often', 'generate', 'limited', 'redundant', 'expansions,', 'potentially', 'constraining', 'effectiveness', 'capturing', 'diverse', 'intents.', 'paper,', 'propose', 'gencrf:', 'generative', 'clustering', 'reformulation', 'framework', 'capture', 'diverse', 'intentions', 'adaptively', 'based', 'multiple', 'differentiated,', 'well-generated', 'queries', 'retrieval', 'phase', 'first', 'time.', 'gencrf', 'leverages', 'llms', 'generate', 'variable', 'queries', 'initial', 'query', 'using', 'customized', 'prompts,', 'clusters', 'groups', 'distinctly', 'represent', 'diverse', 'intents.', 'furthermore,', 'framework', 'explores', 'combine', 'diverse', 'intents', 'query', 'innovative', 'weighted', 'aggregation', 'strategies', 'optimize', 'retrieval', 'performance', 'crucially', 'integrates', 'novel', 'query', 'evaluation', 'rewarding', 'model', '(qerm)', 'refine', 'process', 'feedback', 'loops.', 'empirical', 'experiments', 'beir', 'benchmark', 'demonstrate', 'gencrf', 'achieves', 'state-of-the-art', 'performance,', 'surpassing', 'previous', 'query', 'reformulation', 'sotas', '12%', 'ndcg@10.', 'techniques', 'adapted', 'various', 'llms,', 'significantly', 'boosting', 'retriever', 'performance', 'advancing', 'field', 'information', 'retrieval.'], 'D5': ['session', 'search', 'involves', 'series', 'interactive', 'queries', 'actions', 'fulfill', \"user's\", 'complex', 'information', 'need.', 'current', 'strategies', 'typically', 'prioritize', 'sequential', 'modeling', 'deep', 'semantic', 'understanding,', 'overlooking', 'graph', 'structure', 'interactions.', 'approaches', 'focus', 'capturing', 'structural', 'information,', 'use', 'generalized', 'representation', 'documents,', 'neglecting', 'word-level', 'semantic', 'modeling.', 'paper,', 'propose', 'symbolic', 'graph', 'ranker', '(sgr),', 'aims', 'take', 'advantage', 'text-based', 'graph-based', 'approaches', 'leveraging', 'power', 'recent', 'large', 'language', 'models', '(llms).', 'concretely,', 'first', 'introduce', 'set', 'symbolic', 'grammar', 'rules', 'convert', 'session', 'graph', 'text.', 'allows', 'integrating', 'session', 'history,', 'interaction', 'process,', 'task', 'instruction', 'seamlessly', 'inputs', 'llm.', 'moreover,', 'given', 'natural', 'discrepancy', 'llms', 'pre-trained', 'textual', 'corpora,', 'symbolic', 'language', 'produce', 'using', 'graph-to-text', 'grammar,', 'objective', 'enhance', \"llms'\", 'ability', 'capture', 'graph', 'structures', 'within', 'textual', 'format.', 'achieve', 'this,', 'introduce', 'set', 'self-supervised', 'symbolic', 'learning', 'tasks', 'including', 'link', 'prediction,', 'node', 'content', 'generation,', 'generative', 'contrastive', 'learning,', 'enable', 'llms', 'capture', 'topological', 'information', 'coarse-grained', 'fine-grained.', 'experiment', 'results', 'comprehensive', 'analysis', 'two', 'benchmark', 'datasets,', 'aol', 'tiangong-st,', 'confirm', 'superiority', 'approach.', 'paradigm', 'also', 'offers', 'novel', 'effective', 'methodology', 'bridges', 'gap', 'traditional', 'search', 'strategies', 'modern', 'llms.'], 'D6': ['description', 'item', 'plays', 'pivotal', 'role', 'providing', 'concise', 'informative', 'summaries', 'captivate', 'potential', 'viewers', 'essential', 'recommendation', 'systems.', 'traditionally,', 'descriptions', 'obtained', 'manual', 'web', 'scraping', 'techniques,', 'time-consuming', 'susceptible', 'data', 'inconsistencies.', 'recent', 'years,', 'large', 'language', 'models', '(llms),', 'gpt-3.5,', 'open', 'source', 'llms', 'like', 'alpaca', 'emerged', 'powerful', 'tools', 'natural', 'language', 'processing', 'tasks.', 'paper,', 'explored', 'use', 'llms', 'generate', 'detailed', 'descriptions', 'items.', 'conduct', 'study,', 'used', 'movielens', '1m', 'dataset', 'comprising', 'movie', 'titles', 'goodreads', 'dataset', 'consisting', 'names', 'books', 'subsequently,', 'open-sourced', 'llm,', 'alpaca,', 'prompted', 'few-shot', 'prompting', 'dataset', 'generate', 'detailed', 'movie', 'descriptions', 'considering', 'multiple', 'features', 'like', 'names', 'cast', 'directors', 'ml', 'dataset', 'names', 'author', 'publisher', 'goodreads', 'dataset.', 'generated', 'description', 'compared', 'scraped', 'descriptions', 'using', 'combination', 'top', 'hits,', 'mrr,', 'ndcg', 'evaluation', 'metrics.', 'results', 'demonstrated', 'llm-based', 'movie', 'description', 'generation', 'exhibits', 'significant', 'promise,', 'results', 'comparable', 'ones', 'obtained', 'web-scraped', 'descriptions.']}\n"
     ]
    }
   ],
   "source": [
    "MotsVides = stopwords.words('english')\n",
    "TermesSansMotsVides = {file_name: [word for word in txt if word not in MotsVides] for file_name, txt in termes.items()}\n",
    "print(TermesSansMotsVides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D1': ['queri', 'reformul', '(qr)', 'set', 'techniqu', 'use', 'transform', 'user’', 'origin', 'search', 'queri', 'text', 'better', 'align', 'user’', 'intent', 'improv', 'search', 'experience.', 'recently,', 'zero-shot', 'qr', 'shown', 'promis', 'approach', 'due', 'abil', 'exploit', 'knowledg', 'inher', 'larg', 'languag', 'models.', 'take', 'inspir', 'success', 'ensembl', 'prompt', 'strategi', 'benefit', 'mani', 'tasks,', 'investig', 'help', 'improv', 'queri', 'reformulation.', 'context,', 'propos', 'ensembl', 'base', 'prompt', 'technique,', 'genqrensembl', 'leverag', 'paraphras', 'zero-shot', 'instruct', 'gener', 'multipl', 'set', 'keyword', 'ultim', 'improv', 'retriev', 'performance.', 'introduc', 'post-retriev', 'variant,', 'genqrensemblerf', 'incorpor', 'pseudo', 'relev', 'feedback.', 'evalu', 'four', 'ir', 'benchmarks,', 'find', 'genqrensembl', 'gener', 'better', 'reformul', 'rel', 'ndcg@10', 'improv', '18%', 'map', 'improv', 'upto', '24%', 'previou', 'zero-shot', 'state-of-art.', 'msmarco', 'passag', 'rank', 'task,', 'genqrensemblerf', 'show', 'rel', 'gain', '5%', 'mrr', 'use', 'pseudo-relev', 'feedback,', '9%', 'ndcg@10', 'use', 'relev', 'feedback', 'documents.'], 'D2': ['rank', 'document', 'use', 'larg', 'languag', 'model', '(llms)', 'directli', 'feed', 'queri', 'candid', 'document', 'prompt', 'interest', 'practic', 'problem.', 'however,', 'research', 'found', 'difficult', 'outperform', 'fine-tun', 'baselin', 'ranker', 'benchmark', 'datasets.', 'analyz', 'pointwis', 'listwis', 'rank', 'prompt', 'use', 'exist', 'method', 'argu', 'off-the-shelf', 'llm', 'fulli', 'understand', 'challeng', 'rank', 'formulations.', 'paper,', 'propos', 'significantli', 'reduc', 'burden', 'llm', 'use', 'new', 'techniqu', 'call', 'pairwis', 'rank', 'prompt', '(prp).', 'result', 'first', 'literatur', 'achiev', 'state-of-the-art', 'rank', 'perform', 'standard', 'benchmark', 'use', 'moderate-s', 'open-sourc', 'llms.', 'trec-dl', '2019', '2020,', 'prp', 'base', 'flan-ul2', 'model', '20b', 'paramet', 'perform', 'favor', 'previou', 'best', 'approach', 'literature,', 'base', 'blackbox', 'commerci', 'gpt-4', '50x', '(estimated)', 'model', 'size,', 'outperform', 'llm-base', 'solutions,', 'instructgpt', '175b', 'parameters,', '12-10%', 'rank', 'metrics.', 'use', 'prompt', 'templat', 'seven', 'beir', 'tasks,', 'prp', 'outperform', 'supervis', 'baselin', 'outperform', 'blackbox', 'commerci', 'chatgpt', 'solut', '4.2%', 'pointwis', 'llm-base', 'solut', '10%', 'averag', 'ndcg@10.', 'furthermore,', 'propos', 'sever', 'variant', 'prp', 'improv', 'effici', 'show', 'possibl', 'achiev', 'competit', 'result', 'even', 'linear', 'complex'], 'D3': ['larg', 'languag', 'model', '(llm)', 'manifest', 'unparallel', 'model', 'capabl', 'variou', 'tasks,', 'e.g.,', 'multi-step', 'reasoning,', 'input', 'model', 'mostli', 'limit', 'plain', 'text,', 'could', 'long', 'contain', 'noisi', 'information.', 'long', 'text', 'could', 'take', 'long', 'time', 'process,', 'thu', 'may', 'effici', 'enough', 'recommend', 'system', 'requir', 'immedi', 'response.', 'llm-base', 'recommend', 'models,', 'user', 'item', 'id', 'usual', 'fill', 'templat', '(i.e.,', 'discret', 'prompt)', 'allow', 'model', 'understand', 'given', 'task,', 'model', 'usual', 'need', 'extens', 'fine-tun', 'bridg', 'user/item', 'id', 'templat', 'word', 'unleash', 'power', 'llm', 'recommendation.', 'address', 'problems,', 'propos', 'distil', 'discret', 'prompt', 'specif', 'task', 'set', 'continu', 'prompt', 'vector', 'bridg', 'id', 'word', 'reduc', 'infer', 'time.', 'also', 'design', 'train', 'strategi', 'attempt', 'improv', 'effici', 'train', 'models.', 'experiment', 'result', 'three', 'real-world', 'dataset', 'demonstr', 'effect', 'prompt', 'distil', '(pod)', 'approach', 'sequenti', 'recommend', 'top-n', 'recommend', 'tasks.', 'although', 'train', 'effici', 'significantli', 'improved,', 'improv', 'infer', 'effici', 'limited.', 'find', 'may', 'inspir', 'research', 'commun', 'improv', 'infer', 'effici', 'llm-base', 'recommend', 'models.'], 'D4': ['queri', 'reformul', 'well-known', 'problem', 'inform', 'retriev', '(ir)', 'aim', 'enhanc', 'singl', 'search', 'success', 'complet', 'rate', 'automat', 'modifi', \"user'\", 'input', 'query.', 'recent', 'method', 'leverag', 'larg', 'languag', 'model', '(llms)', 'improv', 'queri', 'reformulation,', 'often', 'gener', 'limit', 'redund', 'expansions,', 'potenti', 'constrain', 'effect', 'captur', 'divers', 'intents.', 'paper,', 'propos', 'gencrf:', 'gener', 'cluster', 'reformul', 'framework', 'captur', 'divers', 'intent', 'adapt', 'base', 'multipl', 'differentiated,', 'well-gener', 'queri', 'retriev', 'phase', 'first', 'time.', 'gencrf', 'leverag', 'llm', 'gener', 'variabl', 'queri', 'initi', 'queri', 'use', 'custom', 'prompts,', 'cluster', 'group', 'distinctli', 'repres', 'divers', 'intents.', 'furthermore,', 'framework', 'explor', 'combin', 'divers', 'intent', 'queri', 'innov', 'weight', 'aggreg', 'strategi', 'optim', 'retriev', 'perform', 'crucial', 'integr', 'novel', 'queri', 'evalu', 'reward', 'model', '(qerm)', 'refin', 'process', 'feedback', 'loops.', 'empir', 'experi', 'beir', 'benchmark', 'demonstr', 'gencrf', 'achiev', 'state-of-the-art', 'performance,', 'surpass', 'previou', 'queri', 'reformul', 'sota', '12%', 'ndcg@10.', 'techniqu', 'adapt', 'variou', 'llms,', 'significantli', 'boost', 'retriev', 'perform', 'advanc', 'field', 'inform', 'retrieval.'], 'D5': ['session', 'search', 'involv', 'seri', 'interact', 'queri', 'action', 'fulfil', \"user'\", 'complex', 'inform', 'need.', 'current', 'strategi', 'typic', 'priorit', 'sequenti', 'model', 'deep', 'semant', 'understanding,', 'overlook', 'graph', 'structur', 'interactions.', 'approach', 'focu', 'captur', 'structur', 'information,', 'use', 'gener', 'represent', 'documents,', 'neglect', 'word-level', 'semant', 'modeling.', 'paper,', 'propos', 'symbol', 'graph', 'ranker', '(sgr),', 'aim', 'take', 'advantag', 'text-bas', 'graph-bas', 'approach', 'leverag', 'power', 'recent', 'larg', 'languag', 'model', '(llms).', 'concretely,', 'first', 'introduc', 'set', 'symbol', 'grammar', 'rule', 'convert', 'session', 'graph', 'text.', 'allow', 'integr', 'session', 'history,', 'interact', 'process,', 'task', 'instruct', 'seamlessli', 'input', 'llm.', 'moreover,', 'given', 'natur', 'discrep', 'llm', 'pre-train', 'textual', 'corpora,', 'symbol', 'languag', 'produc', 'use', 'graph-to-text', 'grammar,', 'object', 'enhanc', \"llms'\", 'abil', 'captur', 'graph', 'structur', 'within', 'textual', 'format.', 'achiev', 'this,', 'introduc', 'set', 'self-supervis', 'symbol', 'learn', 'task', 'includ', 'link', 'prediction,', 'node', 'content', 'generation,', 'gener', 'contrast', 'learning,', 'enabl', 'llm', 'captur', 'topolog', 'inform', 'coarse-grain', 'fine-grained.', 'experi', 'result', 'comprehens', 'analysi', 'two', 'benchmark', 'datasets,', 'aol', 'tiangong-st,', 'confirm', 'superior', 'approach.', 'paradigm', 'also', 'offer', 'novel', 'effect', 'methodolog', 'bridg', 'gap', 'tradit', 'search', 'strategi', 'modern', 'llms.'], 'D6': ['descript', 'item', 'play', 'pivot', 'role', 'provid', 'concis', 'inform', 'summari', 'captiv', 'potenti', 'viewer', 'essenti', 'recommend', 'systems.', 'traditionally,', 'descript', 'obtain', 'manual', 'web', 'scrape', 'techniques,', 'time-consum', 'suscept', 'data', 'inconsistencies.', 'recent', 'years,', 'larg', 'languag', 'model', '(llms),', 'gpt-3.5,', 'open', 'sourc', 'llm', 'like', 'alpaca', 'emerg', 'power', 'tool', 'natur', 'languag', 'process', 'tasks.', 'paper,', 'explor', 'use', 'llm', 'gener', 'detail', 'descript', 'items.', 'conduct', 'study,', 'use', 'movielen', '1m', 'dataset', 'compris', 'movi', 'titl', 'goodread', 'dataset', 'consist', 'name', 'book', 'subsequently,', 'open-sourc', 'llm,', 'alpaca,', 'prompt', 'few-shot', 'prompt', 'dataset', 'gener', 'detail', 'movi', 'descript', 'consid', 'multipl', 'featur', 'like', 'name', 'cast', 'director', 'ml', 'dataset', 'name', 'author', 'publish', 'goodread', 'dataset.', 'gener', 'descript', 'compar', 'scrape', 'descript', 'use', 'combin', 'top', 'hits,', 'mrr,', 'ndcg', 'evalu', 'metrics.', 'result', 'demonstr', 'llm-base', 'movi', 'descript', 'gener', 'exhibit', 'signific', 'promise,', 'result', 'compar', 'one', 'obtain', 'web-scrap', 'descriptions.']}\n",
      "{'D1': ['query', 'reform', '(qr)', 'set', 'techn', 'us', 'transform', 'user’s', 'origin', 'search', 'query', 'text', 'bet', 'align', 'user’s', 'int', 'improv', 'search', 'experience.', 'recently,', 'zero-shot', 'qr', 'shown', 'prom', 'approach', 'due', 'abl', 'exploit', 'knowledg', 'inh', 'larg', 'langu', 'models.', 'tak', 'inspir', 'success', 'ensembl', 'prompt', 'strategies', 'benefit', 'many', 'tasks,', 'investig', 'help', 'improv', 'query', 'reformulation.', 'context,', 'propos', 'ensembl', 'bas', 'prompt', 'technique,', 'genqrensembl', 'lev', 'paraphras', 'zero-shot', 'instruct', 'gen', 'multipl', 'set', 'keyword', 'ultim', 'improv', 'retriev', 'performance.', 'introduc', 'post-retrieval', 'variant,', 'genqrensemblerf', 'incorp', 'pseudo', 'relev', 'feedback.', 'evalu', 'four', 'ir', 'benchmarks,', 'find', 'genqrensembl', 'gen', 'bet', 'reform', 'rel', 'ndcg@10', 'improv', '18%', 'map', 'improv', 'upto', '24%', 'prevy', 'zero-shot', 'state-of-art.', 'msmarco', 'pass', 'rank', 'task,', 'genqrensemblerf', 'show', 'rel', 'gain', '5%', 'mrr', 'us', 'pseudo-relevance', 'feedback,', '9%', 'ndcg@10', 'us', 'relev', 'feedback', 'documents.'], 'D2': ['rank', 'docu', 'us', 'larg', 'langu', 'model', '(llms)', 'direct', 'fee', 'query', 'candid', 'docu', 'prompt', 'interest', 'pract', 'problem.', 'however,', 'research', 'found', 'difficult', 'outperform', 'fine-tuned', 'baselin', 'rank', 'benchmark', 'datasets.', 'analys', 'pointw', 'listw', 'rank', 'prompt', 'us', 'ex', 'method', 'argu', 'off-the-shelf', 'llms', 'ful', 'understand', 'challeng', 'rank', 'formulations.', 'paper,', 'propos', 'sign', 'reduc', 'burd', 'llms', 'us', 'new', 'techn', 'cal', 'pairw', 'rank', 'prompt', '(prp).', 'result', 'first', 'lit', 'achiev', 'state-of-the-art', 'rank', 'perform', 'standard', 'benchmark', 'us', 'moderate-sized', 'open-sourced', 'llms.', 'trec-dl', '2019', '2020,', 'prp', 'bas', 'flan-ul2', 'model', '20b', 'paramet', 'perform', 'fav', 'prevy', 'best', 'approach', 'literature,', 'bas', 'blackbox', 'commerc', 'gpt-4', '50x', '(estimated)', 'model', 'size,', 'outperform', 'llm-based', 'solutions,', 'instructgpt', '175b', 'parameters,', '12-10%', 'rank', 'metrics.', 'us', 'prompt', 'templ', 'sev', 'beir', 'tasks,', 'prp', 'outperform', 'superv', 'baselin', 'outperform', 'blackbox', 'commerc', 'chatgpt', 'solv', '4.2%', 'pointw', 'llm-based', 'solv', '10%', 'av', 'ndcg@10.', 'furthermore,', 'propos', 'sev', 'vary', 'prp', 'improv', 'efficy', 'show', 'poss', 'achiev', 'competit', 'result', 'ev', 'linear', 'complex'], 'D3': ['larg', 'langu', 'model', '(llm)', 'manifest', 'unparallel', 'model', 'cap', 'vary', 'tasks,', 'e.g.,', 'multi-step', 'reasoning,', 'input', 'model', 'most', 'limit', 'plain', 'text,', 'could', 'long', 'contain', 'noisy', 'information.', 'long', 'text', 'could', 'tak', 'long', 'tim', 'process,', 'thu', 'may', 'efficy', 'enough', 'recommend', 'system', 'requir', 'immedy', 'response.', 'llm-based', 'recommend', 'models,', 'us', 'item', 'id', 'us', 'fil', 'templ', '(i.e.,', 'discret', 'prompt)', 'allow', 'model', 'understand', 'giv', 'task,', 'model', 'us', 'nee', 'extend', 'fine-tuning', 'bridg', 'user/item', 'id', 'templ', 'word', 'unleash', 'pow', 'llm', 'recommendation.', 'address', 'problems,', 'propos', 'distil', 'discret', 'prompt', 'spec', 'task', 'set', 'continu', 'prompt', 'vect', 'bridg', 'id', 'word', 'reduc', 'inf', 'time.', 'also', 'design', 'train', 'strategy', 'attempt', 'improv', 'efficy', 'train', 'models.', 'expery', 'result', 'three', 'real-world', 'dataset', 'demonst', 'effect', 'prompt', 'distil', '(pod)', 'approach', 'sequ', 'recommend', 'top-n', 'recommend', 'tasks.', 'although', 'train', 'efficy', 'sign', 'improved,', 'improv', 'inf', 'efficy', 'limited.', 'find', 'may', 'inspir', 'research', 'commun', 'improv', 'inf', 'efficy', 'llm-based', 'recommend', 'models.'], 'D4': ['query', 'reform', 'well-known', 'problem', 'inform', 'retriev', '(ir)', 'aim', 'enh', 'singl', 'search', 'success', 'complet', 'rat', 'autom', 'mod', \"user's\", 'input', 'query.', 'rec', 'method', 'lev', 'larg', 'langu', 'model', '(llms)', 'improv', 'query', 'reformulation,', 'oft', 'gen', 'limit', 'redund', 'expansions,', 'pot', 'constrain', 'effect', 'capt', 'divers', 'intents.', 'paper,', 'propos', 'gencrf:', 'gen', 'clust', 'reform', 'framework', 'capt', 'divers', 'int', 'adapt', 'bas', 'multipl', 'differentiated,', 'well-generated', 'query', 'retriev', 'phas', 'first', 'time.', 'gencrf', 'lev', 'llms', 'gen', 'vary', 'query', 'init', 'query', 'us', 'custom', 'prompts,', 'clust', 'group', 'distinct', 'repres', 'divers', 'intents.', 'furthermore,', 'framework', 'expl', 'combin', 'divers', 'int', 'query', 'innov', 'weight', 'aggreg', 'strategies', 'optim', 'retriev', 'perform', 'cruc', 'integr', 'novel', 'query', 'evalu', 'reward', 'model', '(qerm)', 'refin', 'process', 'feedback', 'loops.', 'empir', 'expery', 'beir', 'benchmark', 'demonst', 'gencrf', 'achiev', 'state-of-the-art', 'performance,', 'surpass', 'prevy', 'query', 'reform', 'sota', '12%', 'ndcg@10.', 'techn', 'adapt', 'vary', 'llms,', 'sign', 'boost', 'retriev', 'perform', 'adv', 'field', 'inform', 'retrieval.'], 'D5': ['sess', 'search', 'involv', 'sery', 'interact', 'query', 'act', 'fulfil', \"user's\", 'complex', 'inform', 'need.', 'cur', 'strategies', 'typ', 'priorit', 'sequ', 'model', 'deep', 'sem', 'understanding,', 'overlook', 'graph', 'structure', 'interactions.', 'approach', 'foc', 'capt', 'structural', 'information,', 'us', 'gen', 'repres', 'documents,', 'neglect', 'word-level', 'sem', 'modeling.', 'paper,', 'propos', 'symbol', 'graph', 'rank', '(sgr),', 'aim', 'tak', 'adv', 'text-based', 'graph-based', 'approach', 'lev', 'pow', 'rec', 'larg', 'langu', 'model', '(llms).', 'concretely,', 'first', 'introduc', 'set', 'symbol', 'gramm', 'rul', 'convert', 'sess', 'graph', 'text.', 'allow', 'integr', 'sess', 'history,', 'interact', 'process,', 'task', 'instruct', 'seamless', 'input', 'llm.', 'moreover,', 'giv', 'nat', 'discrep', 'llms', 'pre-trained', 'text', 'corpora,', 'symbol', 'langu', 'produc', 'us', 'graph-to-text', 'grammar,', 'object', 'enh', \"llms'\", 'abl', 'capt', 'graph', 'structures', 'within', 'text', 'format.', 'achiev', 'this,', 'introduc', 'set', 'self-supervised', 'symbol', 'learn', 'task', 'includ', 'link', 'prediction,', 'nod', 'cont', 'generation,', 'gen', 'contrast', 'learning,', 'en', 'llms', 'capt', 'topolog', 'inform', 'coarse-grained', 'fine-grained.', 'expery', 'result', 'comprehend', 'analys', 'two', 'benchmark', 'datasets,', 'aol', 'tiangong-st,', 'confirm', 'supery', 'approach.', 'paradigm', 'also', 'off', 'novel', 'effect', 'methodolog', 'bridg', 'gap', 'tradit', 'search', 'strategies', 'modern', 'llms.'], 'D6': ['describ', 'item', 'play', 'pivot', 'rol', 'provid', 'cont', 'inform', 'sum', 'capt', 'pot', 'view', 'ess', 'recommend', 'systems.', 'traditionally,', 'describ', 'obtain', 'man', 'web', 'scraping', 'techniques,', 'time-consuming', 'suscept', 'dat', 'inconsistencies.', 'rec', 'years,', 'larg', 'langu', 'model', '(llms),', 'gpt-3.5,', 'op', 'sourc', 'llms', 'lik', 'alpac', 'emerg', 'pow', 'tool', 'nat', 'langu', 'process', 'tasks.', 'paper,', 'expl', 'us', 'llms', 'gen', 'detail', 'describ', 'items.', 'conduc', 'study,', 'us', 'moviel', '1m', 'dataset', 'compr', 'movy', 'titl', 'goodread', 'dataset', 'consist', 'nam', 'book', 'subsequently,', 'open-sourced', 'llm,', 'alpaca,', 'prompt', 'few-shot', 'prompt', 'dataset', 'gen', 'detail', 'movy', 'describ', 'consid', 'multipl', 'feat', 'lik', 'nam', 'cast', 'direct', 'ml', 'dataset', 'nam', 'auth', 'publ', 'goodread', 'dataset.', 'gen', 'describ', 'comp', 'scraped', 'describ', 'us', 'combin', 'top', 'hits,', 'mrr,', 'ndcg', 'evalu', 'metrics.', 'result', 'demonst', 'llm-based', 'movy', 'describ', 'gen', 'exhibit', 'sign', 'promise,', 'result', 'comp', 'on', 'obtain', 'web-scraped', 'descriptions.']}\n"
     ]
    }
   ],
   "source": [
    "TermesNormalisationPorter = {file_name: [Porter.stem(word) for word in txt] for file_name, txt in TermesSansMotsVides.items()}\n",
    "print(TermesNormalisationPorter)\n",
    "TermesNormalisationLancaster = {file_name: [Lancaster.stem(word) for word in txt] for file_name, txt in TermesSansMotsVides.items()}\n",
    "print(TermesNormalisationLancaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. (qr)', '1. 18%', '1. 24%', '1. 5%', '1. 9%', '1. abil', '1. align', '1. approach', '1. base', '1. benchmarks,', '1. benefit', '1. better', '1. context,', '1. documents.', '1. due', '1. ensembl', '1. evalu', '1. experience.', '1. exploit', '1. feedback', '1. feedback,', '1. feedback.', '1. find', '1. four', '1. gain', '1. gener', '1. genqrensembl', '1. genqrensemblerf', '1. help', '1. improv', '1. incorpor', '1. inher', '1. inspir', '1. instruct', '1. intent', '1. introduc', '1. investig', '1. ir', '1. keyword', '1. knowledg', '1. languag', '1. larg', '1. leverag', '1. mani', '1. map', '1. models.', '1. mrr', '1. msmarco', '1. multipl', '1. ndcg@10', '1. origin', '1. paraphras', '1. passag', '1. performance.', '1. post-retriev', '1. previou', '1. promis', '1. prompt', '1. propos', '1. pseudo', '1. pseudo-relev', '1. qr', '1. queri', '1. rank', '1. recently,', '1. reformul', '1. reformulation.', '1. rel', '1. relev', '1. retriev', '1. search', '1. set', '1. show', '1. shown', '1. state-of-art.', '1. strategi', '1. success', '1. take', '1. task,', '1. tasks,', '1. techniqu', '1. technique,', '1. text', '1. transform', '1. ultim', '1. upto', '1. use', '1. user’', '1. variant,', '1. zero-shot', '2. (estimated)', '2. (llms)', '2. (prp).', '2. 10%', '2. 12-10%', '2. 175b', '2. 2019', '2. 2020,', '2. 20b', '2. 4.2%', '2. 50x', '2. achiev', '2. analyz', '2. approach', '2. argu', '2. averag', '2. base', '2. baselin', '2. beir', '2. benchmark', '2. best', '2. blackbox', '2. burden', '2. call', '2. candid', '2. challeng', '2. chatgpt', '2. commerci', '2. competit', '2. complex', '2. datasets.', '2. difficult', '2. directli', '2. document', '2. effici', '2. even', '2. exist', '2. favor', '2. feed', '2. fine-tun', '2. first', '2. flan-ul2', '2. formulations.', '2. found', '2. fulli', '2. furthermore,', '2. gpt-4', '2. however,', '2. improv', '2. instructgpt', '2. interest', '2. languag', '2. larg', '2. linear', '2. listwis', '2. literatur', '2. literature,', '2. llm', '2. llm-base', '2. llms.', '2. method', '2. metrics.', '2. model', '2. moderate-s', '2. ndcg@10.', '2. new', '2. off-the-shelf', '2. open-sourc', '2. outperform', '2. pairwis', '2. paper,', '2. paramet', '2. parameters,', '2. perform', '2. pointwis', '2. possibl', '2. practic', '2. previou', '2. problem.', '2. prompt', '2. propos', '2. prp', '2. queri', '2. rank', '2. ranker', '2. reduc', '2. research', '2. result', '2. seven', '2. sever', '2. show', '2. significantli', '2. size,', '2. solut', '2. solutions,', '2. standard', '2. state-of-the-art', '2. supervis', '2. tasks,', '2. techniqu', '2. templat', '2. trec-dl', '2. understand', '2. use', '2. variant', '3. (i.e.,', '3. (llm)', '3. (pod)', '3. address', '3. allow', '3. also', '3. although', '3. approach', '3. attempt', '3. bridg', '3. capabl', '3. commun', '3. contain', '3. continu', '3. could', '3. dataset', '3. demonstr', '3. design', '3. discret', '3. distil', '3. e.g.,', '3. effect', '3. effici', '3. enough', '3. experiment', '3. extens', '3. fill', '3. find', '3. fine-tun', '3. given', '3. id', '3. immedi', '3. improv', '3. improved,', '3. infer', '3. information.', '3. input', '3. inspir', '3. item', '3. languag', '3. larg', '3. limit', '3. limited.', '3. llm', '3. llm-base', '3. long', '3. manifest', '3. may', '3. model', '3. models,', '3. models.', '3. mostli', '3. multi-step', '3. need', '3. noisi', '3. plain', '3. power', '3. problems,', '3. process,', '3. prompt', '3. prompt)', '3. propos', '3. real-world', '3. reasoning,', '3. recommend', '3. recommendation.', '3. reduc', '3. requir', '3. research', '3. response.', '3. result', '3. sequenti', '3. set', '3. significantli', '3. specif', '3. strategi', '3. system', '3. take', '3. task', '3. task,', '3. tasks,', '3. tasks.', '3. templat', '3. text', '3. text,', '3. three', '3. thu', '3. time', '3. time.', '3. top-n', '3. train', '3. understand', '3. unleash', '3. unparallel', '3. user', '3. user/item', '3. usual', '3. variou', '3. vector', '3. word', '4. (ir)', '4. (llms)', '4. (qerm)', '4. 12%', '4. achiev', '4. adapt', '4. advanc', '4. aggreg', '4. aim', '4. automat', '4. base', '4. beir', '4. benchmark', '4. boost', '4. captur', '4. cluster', '4. combin', '4. complet', '4. constrain', '4. crucial', '4. custom', '4. demonstr', '4. differentiated,', '4. distinctli', '4. divers', '4. effect', '4. empir', '4. enhanc', '4. evalu', '4. expansions,', '4. experi', '4. explor', '4. feedback', '4. field', '4. first', '4. framework', '4. furthermore,', '4. gencrf', '4. gencrf:', '4. gener', '4. group', '4. improv', '4. inform', '4. initi', '4. innov', '4. input', '4. integr', '4. intent', '4. intents.', '4. languag', '4. larg', '4. leverag', '4. limit', '4. llm', '4. llms,', '4. loops.', '4. method', '4. model', '4. modifi', '4. multipl', '4. ndcg@10.', '4. novel', '4. often', '4. optim', '4. paper,', '4. perform', '4. performance,', '4. phase', '4. potenti', '4. previou', '4. problem', '4. process', '4. prompts,', '4. propos', '4. queri', '4. query.', '4. rate', '4. recent', '4. redund', '4. refin', '4. reformul', '4. reformulation,', '4. repres', '4. retriev', '4. retrieval.', '4. reward', '4. search', '4. significantli', '4. singl', '4. sota', '4. state-of-the-art', '4. strategi', '4. success', '4. surpass', '4. techniqu', '4. time.', '4. use', \"4. user'\", '4. variabl', '4. variou', '4. weight', '4. well-gener', '4. well-known', '5. (llms).', '5. (sgr),', '5. abil', '5. achiev', '5. action', '5. advantag', '5. aim', '5. allow', '5. also', '5. analysi', '5. aol', '5. approach', '5. approach.', '5. benchmark', '5. bridg', '5. captur', '5. coarse-grain', '5. complex', '5. comprehens', '5. concretely,', '5. confirm', '5. content', '5. contrast', '5. convert', '5. corpora,', '5. current', '5. datasets,', '5. deep', '5. discrep', '5. documents,', '5. effect', '5. enabl', '5. enhanc', '5. experi', '5. fine-grained.', '5. first', '5. focu', '5. format.', '5. fulfil', '5. gap', '5. gener', '5. generation,', '5. given', '5. grammar', '5. grammar,', '5. graph', '5. graph-bas', '5. graph-to-text', '5. history,', '5. includ', '5. inform', '5. information,', '5. input', '5. instruct', '5. integr', '5. interact', '5. interactions.', '5. introduc', '5. involv', '5. languag', '5. larg', '5. learn', '5. learning,', '5. leverag', '5. link', '5. llm', '5. llm.', \"5. llms'\", '5. llms.', '5. methodolog', '5. model', '5. modeling.', '5. modern', '5. moreover,', '5. natur', '5. need.', '5. neglect', '5. node', '5. novel', '5. object', '5. offer', '5. overlook', '5. paper,', '5. paradigm', '5. power', '5. pre-train', '5. prediction,', '5. priorit', '5. process,', '5. produc', '5. propos', '5. queri', '5. ranker', '5. recent', '5. represent', '5. result', '5. rule', '5. seamlessli', '5. search', '5. self-supervis', '5. semant', '5. sequenti', '5. seri', '5. session', '5. set', '5. strategi', '5. structur', '5. superior', '5. symbol', '5. take', '5. task', '5. text-bas', '5. text.', '5. textual', '5. this,', '5. tiangong-st,', '5. topolog', '5. tradit', '5. two', '5. typic', '5. understanding,', '5. use', \"5. user'\", '5. within', '5. word-level', '6. (llms),', '6. 1m', '6. alpaca', '6. alpaca,', '6. author', '6. book', '6. captiv', '6. cast', '6. combin', '6. compar', '6. compris', '6. concis', '6. conduct', '6. consid', '6. consist', '6. data', '6. dataset', '6. dataset.', '6. demonstr', '6. descript', '6. descriptions.', '6. detail', '6. director', '6. emerg', '6. essenti', '6. evalu', '6. exhibit', '6. explor', '6. featur', '6. few-shot', '6. gener', '6. goodread', '6. gpt-3.5,', '6. hits,', '6. inconsistencies.', '6. inform', '6. item', '6. items.', '6. languag', '6. larg', '6. like', '6. llm', '6. llm,', '6. llm-base', '6. manual', '6. metrics.', '6. ml', '6. model', '6. movi', '6. movielen', '6. mrr,', '6. multipl', '6. name', '6. natur', '6. ndcg', '6. obtain', '6. one', '6. open', '6. open-sourc', '6. paper,', '6. pivot', '6. play', '6. potenti', '6. power', '6. process', '6. promise,', '6. prompt', '6. provid', '6. publish', '6. recent', '6. recommend', '6. result', '6. role', '6. scrape', '6. signific', '6. sourc', '6. study,', '6. subsequently,', '6. summari', '6. suscept', '6. systems.', '6. tasks.', '6. techniques,', '6. time-consum', '6. titl', '6. tool', '6. top', '6. traditionally,', '6. use', '6. viewer', '6. web', '6. web-scrap', '6. years,']\n",
      "['1. (qr)', '1. 18%', '1. 24%', '1. 5%', '1. 9%', '1. abil', '1. align', '1. approach', '1. base', '1. benchmarks,', '1. benefit', '1. better', '1. context,', '1. documents.', '1. due', '1. ensembl', '1. evalu', '1. experience.', '1. exploit', '1. feedback', '1. feedback,', '1. feedback.', '1. find', '1. four', '1. gain', '1. gener', '1. genqrensembl', '1. genqrensemblerf', '1. help', '1. improv', '1. incorpor', '1. inher', '1. inspir', '1. instruct', '1. intent', '1. introduc', '1. investig', '1. ir', '1. keyword', '1. knowledg', '1. languag', '1. larg', '1. leverag', '1. mani', '1. map', '1. models.', '1. mrr', '1. msmarco', '1. multipl', '1. ndcg@10', '1. origin', '1. paraphras', '1. passag', '1. performance.', '1. post-retriev', '1. previou', '1. promis', '1. prompt', '1. propos', '1. pseudo', '1. pseudo-relev', '1. qr', '1. queri', '1. rank', '1. recently,', '1. reformul', '1. reformulation.', '1. rel', '1. relev', '1. retriev', '1. search', '1. set', '1. show', '1. shown', '1. state-of-art.', '1. strategi', '1. success', '1. take', '1. task,', '1. tasks,', '1. techniqu', '1. technique,', '1. text', '1. transform', '1. ultim', '1. upto', '1. use', '1. user’', '1. variant,', '1. zero-shot', '2. (estimated)', '2. (llms)', '2. (prp).', '2. 10%', '2. 12-10%', '2. 175b', '2. 2019', '2. 2020,', '2. 20b', '2. 4.2%', '2. 50x', '2. achiev', '2. analyz', '2. approach', '2. argu', '2. averag', '2. base', '2. baselin', '2. beir', '2. benchmark', '2. best', '2. blackbox', '2. burden', '2. call', '2. candid', '2. challeng', '2. chatgpt', '2. commerci', '2. competit', '2. complex', '2. datasets.', '2. difficult', '2. directli', '2. document', '2. effici', '2. even', '2. exist', '2. favor', '2. feed', '2. fine-tun', '2. first', '2. flan-ul2', '2. formulations.', '2. found', '2. fulli', '2. furthermore,', '2. gpt-4', '2. however,', '2. improv', '2. instructgpt', '2. interest', '2. languag', '2. larg', '2. linear', '2. listwis', '2. literatur', '2. literature,', '2. llm', '2. llm-base', '2. llms.', '2. method', '2. metrics.', '2. model', '2. moderate-s', '2. ndcg@10.', '2. new', '2. off-the-shelf', '2. open-sourc', '2. outperform', '2. pairwis', '2. paper,', '2. paramet', '2. parameters,', '2. perform', '2. pointwis', '2. possibl', '2. practic', '2. previou', '2. problem.', '2. prompt', '2. propos', '2. prp', '2. queri', '2. rank', '2. ranker', '2. reduc', '2. research', '2. result', '2. seven', '2. sever', '2. show', '2. significantli', '2. size,', '2. solut', '2. solutions,', '2. standard', '2. state-of-the-art', '2. supervis', '2. tasks,', '2. techniqu', '2. templat', '2. trec-dl', '2. understand', '2. use', '2. variant', '3. (i.e.,', '3. (llm)', '3. (pod)', '3. address', '3. allow', '3. also', '3. although', '3. approach', '3. attempt', '3. bridg', '3. capabl', '3. commun', '3. contain', '3. continu', '3. could', '3. dataset', '3. demonstr', '3. design', '3. discret', '3. distil', '3. e.g.,', '3. effect', '3. effici', '3. enough', '3. experiment', '3. extens', '3. fill', '3. find', '3. fine-tun', '3. given', '3. id', '3. immedi', '3. improv', '3. improved,', '3. infer', '3. information.', '3. input', '3. inspir', '3. item', '3. languag', '3. larg', '3. limit', '3. limited.', '3. llm', '3. llm-base', '3. long', '3. manifest', '3. may', '3. model', '3. models,', '3. models.', '3. mostli', '3. multi-step', '3. need', '3. noisi', '3. plain', '3. power', '3. problems,', '3. process,', '3. prompt', '3. prompt)', '3. propos', '3. real-world', '3. reasoning,', '3. recommend', '3. recommendation.', '3. reduc', '3. requir', '3. research', '3. response.', '3. result', '3. sequenti', '3. set', '3. significantli', '3. specif', '3. strategi', '3. system', '3. take', '3. task', '3. task,', '3. tasks,', '3. tasks.', '3. templat', '3. text', '3. text,', '3. three', '3. thu', '3. time', '3. time.', '3. top-n', '3. train', '3. understand', '3. unleash', '3. unparallel', '3. user', '3. user/item', '3. usual', '3. variou', '3. vector', '3. word', '4. (ir)', '4. (llms)', '4. (qerm)', '4. 12%', '4. achiev', '4. adapt', '4. advanc', '4. aggreg', '4. aim', '4. automat', '4. base', '4. beir', '4. benchmark', '4. boost', '4. captur', '4. cluster', '4. combin', '4. complet', '4. constrain', '4. crucial', '4. custom', '4. demonstr', '4. differentiated,', '4. distinctli', '4. divers', '4. effect', '4. empir', '4. enhanc', '4. evalu', '4. expansions,', '4. experi', '4. explor', '4. feedback', '4. field', '4. first', '4. framework', '4. furthermore,', '4. gencrf', '4. gencrf:', '4. gener', '4. group', '4. improv', '4. inform', '4. initi', '4. innov', '4. input', '4. integr', '4. intent', '4. intents.', '4. languag', '4. larg', '4. leverag', '4. limit', '4. llm', '4. llms,', '4. loops.', '4. method', '4. model', '4. modifi', '4. multipl', '4. ndcg@10.', '4. novel', '4. often', '4. optim', '4. paper,', '4. perform', '4. performance,', '4. phase', '4. potenti', '4. previou', '4. problem', '4. process', '4. prompts,', '4. propos', '4. queri', '4. query.', '4. rate', '4. recent', '4. redund', '4. refin', '4. reformul', '4. reformulation,', '4. repres', '4. retriev', '4. retrieval.', '4. reward', '4. search', '4. significantli', '4. singl', '4. sota', '4. state-of-the-art', '4. strategi', '4. success', '4. surpass', '4. techniqu', '4. time.', '4. use', \"4. user'\", '4. variabl', '4. variou', '4. weight', '4. well-gener', '4. well-known', '5. (llms).', '5. (sgr),', '5. abil', '5. achiev', '5. action', '5. advantag', '5. aim', '5. allow', '5. also', '5. analysi', '5. aol', '5. approach', '5. approach.', '5. benchmark', '5. bridg', '5. captur', '5. coarse-grain', '5. complex', '5. comprehens', '5. concretely,', '5. confirm', '5. content', '5. contrast', '5. convert', '5. corpora,', '5. current', '5. datasets,', '5. deep', '5. discrep', '5. documents,', '5. effect', '5. enabl', '5. enhanc', '5. experi', '5. fine-grained.', '5. first', '5. focu', '5. format.', '5. fulfil', '5. gap', '5. gener', '5. generation,', '5. given', '5. grammar', '5. grammar,', '5. graph', '5. graph-bas', '5. graph-to-text', '5. history,', '5. includ', '5. inform', '5. information,', '5. input', '5. instruct', '5. integr', '5. interact', '5. interactions.', '5. introduc', '5. involv', '5. languag', '5. larg', '5. learn', '5. learning,', '5. leverag', '5. link', '5. llm', '5. llm.', \"5. llms'\", '5. llms.', '5. methodolog', '5. model', '5. modeling.', '5. modern', '5. moreover,', '5. natur', '5. need.', '5. neglect', '5. node', '5. novel', '5. object', '5. offer', '5. overlook', '5. paper,', '5. paradigm', '5. power', '5. pre-train', '5. prediction,', '5. priorit', '5. process,', '5. produc', '5. propos', '5. queri', '5. ranker', '5. recent', '5. represent', '5. result', '5. rule', '5. seamlessli', '5. search', '5. self-supervis', '5. semant', '5. sequenti', '5. seri', '5. session', '5. set', '5. strategi', '5. structur', '5. superior', '5. symbol', '5. take', '5. task', '5. text-bas', '5. text.', '5. textual', '5. this,', '5. tiangong-st,', '5. topolog', '5. tradit', '5. two', '5. typic', '5. understanding,', '5. use', \"5. user'\", '5. within', '5. word-level', '6. (llms),', '6. 1m', '6. alpaca', '6. alpaca,', '6. author', '6. book', '6. captiv', '6. cast', '6. combin', '6. compar', '6. compris', '6. concis', '6. conduct', '6. consid', '6. consist', '6. data', '6. dataset', '6. dataset.', '6. demonstr', '6. descript', '6. descriptions.', '6. detail', '6. director', '6. emerg', '6. essenti', '6. evalu', '6. exhibit', '6. explor', '6. featur', '6. few-shot', '6. gener', '6. goodread', '6. gpt-3.5,', '6. hits,', '6. inconsistencies.', '6. inform', '6. item', '6. items.', '6. languag', '6. larg', '6. like', '6. llm', '6. llm,', '6. llm-base', '6. manual', '6. metrics.', '6. ml', '6. model', '6. movi', '6. movielen', '6. mrr,', '6. multipl', '6. name', '6. natur', '6. ndcg', '6. obtain', '6. one', '6. open', '6. open-sourc', '6. paper,', '6. pivot', '6. play', '6. potenti', '6. power', '6. process', '6. promise,', '6. prompt', '6. provid', '6. publish', '6. recent', '6. recommend', '6. result', '6. role', '6. scrape', '6. signific', '6. sourc', '6. study,', '6. subsequently,', '6. summari', '6. suscept', '6. systems.', '6. tasks.', '6. techniques,', '6. time-consum', '6. titl', '6. tool', '6. top', '6. traditionally,', '6. use', '6. viewer', '6. web', '6. web-scrap', '6. years,']\n"
     ]
    }
   ],
   "source": [
    "TermesSortedPorter = {file_name: sorted(txt) for file_name, txt in TermesNormalisationPorter.items()}\n",
    "\n",
    "all_termesPorter = []\n",
    "for idx, (file_name, termes) in enumerate(TermesSortedPorter.items(), start=1):\n",
    "    unique_termesPorter = set(termes)\n",
    "    sorted_termesPorter = sorted(unique_termesPorter)\n",
    "    final_termesPorter = [f'{idx}. {terme}' for terme in sorted_termesPorter]\n",
    "    all_termesPorter += final_termesPorter\n",
    "\n",
    "all_termesPorter = sorted(all_termesPorter)\n",
    "\n",
    "print(all_termesPorter)\n",
    "\n",
    "\n",
    "\n",
    "TermesSortedLancaster = {file_name: sorted(txt) for file_name, txt in TermesNormalisationLancaster.items()}\n",
    "\n",
    "all_termesLancaster = []\n",
    "for idx, (file_name, termes) in enumerate(TermesSortedPorter.items(), start=1):\n",
    "    unique_termesLancaster = set(termes)\n",
    "    sorted_termesLancaster = sorted(unique_termesLancaster)\n",
    "    final_termesLancaster = [f'{idx}. {terme}' for terme in sorted_termesLancaster]\n",
    "    all_termesLancaster += final_termesLancaster\n",
    "\n",
    "all_termesLancaster = sorted(all_termesLancaster)\n",
    "\n",
    "\n",
    "\n",
    "print(all_termesLancaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=\"../Index/DescripteursSplitPorter.txt\", mode='w') as f :\n",
    "    for terme in all_termesPorter:\n",
    "        f.write(f'{terme}\\n')\n",
    "\n",
    "with open(file=\"../Index/DescripteursSplitLancaster.txt\", mode='w') as f :\n",
    "    for terme in all_termesLancaster:\n",
    "        f.write(f'{terme}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_termesPorter = []\n",
    "for idx, (file_name, termes) in enumerate(TermesSortedPorter.items(), start=1):\n",
    "    unique_termesPorter = set(termes)\n",
    "    sorted_termesPorter = sorted(unique_termesPorter)\n",
    "    final_termesPorter = [f'{terme} {idx}' for terme in sorted_termesPorter]\n",
    "    all_termesPorter += final_termesPorter\n",
    "\n",
    "all_termesPorter = sorted(all_termesPorter)\n",
    "\n",
    "all_termesLancaster = []\n",
    "for idx, (file_name, termes) in enumerate(TermesSortedPorter.items(), start=1):\n",
    "    unique_termesLancaster = set(termes)\n",
    "    sorted_termesLancaster = sorted(unique_termesLancaster)\n",
    "    final_termesLancaster = [f'{terme} {idx}' for terme in sorted_termesLancaster]\n",
    "    all_termesLancaster += final_termesLancaster\n",
    "\n",
    "all_termesLancaster = sorted(all_termesLancaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=\"../Index/InverseSplitPorter.txt\", mode='w') as f :\n",
    "    for terme in all_termesPorter:\n",
    "            f.write(f'{terme}\\n')\n",
    "\n",
    "with open(file=\"../Index/InverseSplitLancaster.txt\", mode='w') as f :\n",
    "    for terme in all_termesLancaster:\n",
    "            f.write(f'{terme}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D1': ['query', 'reformulation', 'qr', 'is', 'a', 'set', 'of', 'techniques', 'used', 'to', 'transform', 'a', 'user', 's', 'original', 'search', 'query', 'to', 'a', 'text', 'that', 'better', 'aligns', 'with', 'the', 'user', 's', 'intent', 'and', 'improves', 'their', 'search', 'experience', 'recently', 'zero-shot', 'qr', 'has', 'been', 'shown', 'to', 'be', 'a', 'promising', 'approach', 'due', 'to', 'its', 'ability', 'to', 'exploit', 'knowledge', 'inherent', 'in', 'large', 'language', 'models', 'by', 'taking', 'inspiration', 'from', 'the', 'success', 'of', 'ensemble', 'prompting', 'strategies', 'which', 'have', 'benefited', 'many', 'tasks', 'we', 'investigate', 'if', 'they', 'can', 'help', 'improve', 'query', 'reformulation', 'in', 'this', 'context', 'we', 'propose', 'an', 'ensemble', 'based', 'prompting', 'technique', 'genqrensemble', 'which', 'leverages', 'paraphrases', 'of', 'a', 'zero-shot', 'instruction', 'to', 'generate', 'multiple', 'sets', 'of', 'keywords', 'ultimately', 'improving', 'retrieval', 'performance', 'we', 'further', 'introduce', 'its', 'post-retrieval', 'variant', 'genqrensemblerf', 'to', 'incorporate', 'pseudo', 'relevant', 'feedback', 'on', 'evaluations', 'over', 'four', 'ir', 'benchmarks', 'we', 'find', 'that', 'genqrensemble', 'generates', 'better', 'reformulations', 'with', 'relative', 'ndcg@10', 'improvements', 'up', 'to', '18%', 'and', 'map', 'improvements', 'upto', '24%', 'over', 'the', 'previous', 'zero-shot', 'state-of-art', 'on', 'the', 'msmarco', 'passage', 'ranking', 'task', 'genqrensemblerf', 'shows', 'relative', 'gains', 'of', '5%', 'mrr', 'using', 'pseudo-relevance', 'feedback', 'and', '9%', 'ndcg@10', 'using', 'relevant', 'feedback', 'documents'], 'D2': ['ranking', 'documents', 'using', 'large', 'language', 'models', 'llms', 'by', 'directly', 'feeding', 'the', 'query', 'and', 'candidate', 'documents', 'into', 'the', 'prompt', 'is', 'an', 'interesting', 'and', 'practical', 'problem', 'however', 'researchers', 'have', 'found', 'it', 'difficult', 'to', 'outperform', 'fine-tuned', 'baseline', 'rankers', 'on', 'benchmark', 'datasets', 'we', 'analyze', 'pointwise', 'and', 'listwise', 'ranking', 'prompts', 'used', 'by', 'existing', 'methods', 'and', 'argue', 'that', 'off-the-shelf', 'llms', 'do', 'not', 'fully', 'understand', 'these', 'challenging', 'ranking', 'formulations', 'in', 'this', 'paper', 'we', 'propose', 'to', 'significantly', 'reduce', 'the', 'burden', 'on', 'llms', 'by', 'using', 'a', 'new', 'technique', 'called', 'pairwise', 'ranking', 'prompting', 'prp', 'our', 'results', 'are', 'the', 'first', 'in', 'the', 'literature', 'to', 'achieve', 'state-of-the-art', 'ranking', 'performance', 'on', 'standard', 'benchmarks', 'using', 'moderate-sized', 'open-sourced', 'llms', 'on', 'trec-dl', '2019', 'and', '2020', 'prp', 'based', 'on', 'the', 'flan-ul2', 'model', 'with', '20b', 'parameters', 'performs', 'favorably', 'with', 'the', 'previous', 'best', 'approach', 'in', 'the', 'literature', 'which', 'is', 'based', 'on', 'the', 'blackbox', 'commercial', 'gpt-4', 'that', 'has', '50x', 'estimated', 'model', 'size', 'while', 'outperforming', 'other', 'llm-based', 'solutions', 'such', 'as', 'instructgpt', 'which', 'has', '175b', 'parameters', 'by', 'over', '12-10%', 'for', 'all', 'ranking', 'metrics', 'by', 'using', 'the', 'same', 'prompt', 'template', 'on', 'seven', 'beir', 'tasks', 'prp', 'outperforms', 'supervised', 'baselines', 'and', 'outperforms', 'the', 'blackbox', 'commercial', 'chatgpt', 'solution', 'by', '4.2%', 'and', 'pointwise', 'llm-based', 'solutions', 'by', 'more', 'than', '10%', 'on', 'average', 'ndcg@10', 'furthermore', 'we', 'propose', 'several', 'variants', 'of', 'prp', 'to', 'improve', 'efficiency', 'and', 'show', 'that', 'it', 'is', 'possible', 'to', 'achieve', 'competitive', 'results', 'even', 'with', 'linear', 'complexity'], 'D3': ['large', 'language', 'models', 'llm', 'have', 'manifested', 'unparalleled', 'modeling', 'capability', 'on', 'various', 'tasks', 'e.g.', 'multi-step', 'reasoning', 'but', 'the', 'input', 'to', 'these', 'models', 'is', 'mostly', 'limited', 'to', 'plain', 'text', 'which', 'could', 'be', 'very', 'long', 'and', 'contain', 'noisy', 'information', 'long', 'text', 'could', 'take', 'long', 'time', 'to', 'process', 'and', 'thus', 'may', 'not', 'be', 'efficient', 'enough', 'for', 'recommender', 'systems', 'that', 'require', 'immediate', 'response', 'in', 'llm-based', 'recommendation', 'models', 'user', 'and', 'item', 'ids', 'are', 'usually', 'filled', 'in', 'a', 'template', 'i.e.', 'discrete', 'prompt', 'to', 'allow', 'the', 'models', 'to', 'understand', 'a', 'given', 'task', 'but', 'the', 'models', 'usually', 'need', 'extensive', 'fine-tuning', 'to', 'bridge', 'the', 'user/item', 'ids', 'and', 'the', 'template', 'words', 'and', 'to', 'unleash', 'the', 'power', 'of', 'llm', 'for', 'recommendation', 'to', 'address', 'the', 'problems', 'we', 'propose', 'to', 'distill', 'the', 'discrete', 'prompt', 'for', 'a', 'specific', 'task', 'to', 'a', 'set', 'of', 'continuous', 'prompt', 'vectors', 'so', 'as', 'to', 'bridge', 'ids', 'and', 'words', 'and', 'to', 'reduce', 'the', 'inference', 'time', 'we', 'also', 'design', 'a', 'training', 'strategy', 'with', 'an', 'attempt', 'to', 'improve', 'the', 'efficiency', 'of', 'training', 'these', 'models', 'experimental', 'results', 'on', 'three', 'real-world', 'datasets', 'demonstrate', 'the', 'effectiveness', 'of', 'our', 'prompt', 'distillation', 'pod', 'approach', 'on', 'both', 'sequential', 'recommendation', 'and', 'top-n', 'recommendation', 'tasks', 'although', 'the', 'training', 'efficiency', 'can', 'be', 'significantly', 'improved', 'the', 'improvement', 'of', 'inference', 'efficiency', 'is', 'limited', 'this', 'finding', 'may', 'inspire', 'researchers', 'in', 'the', 'community', 'to', 'further', 'improve', 'the', 'inference', 'efficiency', 'of', 'llm-based', 'recommendation', 'models'], 'D4': ['query', 'reformulation', 'is', 'a', 'well-known', 'problem', 'in', 'information', 'retrieval', 'ir', 'aimed', 'at', 'enhancing', 'single', 'search', 'successful', 'completion', 'rate', 'by', 'automatically', 'modifying', 'user', 's', 'input', 'query', 'recent', 'methods', 'leverage', 'large', 'language', 'models', 'llms', 'to', 'improve', 'query', 'reformulation', 'but', 'often', 'generate', 'limited', 'and', 'redundant', 'expansions', 'potentially', 'constraining', 'their', 'effectiveness', 'in', 'capturing', 'diverse', 'intents', 'in', 'this', 'paper', 'we', 'propose', 'gencrf', 'a', 'generative', 'clustering', 'and', 'reformulation', 'framework', 'to', 'capture', 'diverse', 'intentions', 'adaptively', 'based', 'on', 'multiple', 'differentiated', 'well-generated', 'queries', 'in', 'the', 'retrieval', 'phase', 'for', 'the', 'first', 'time', 'gencrf', 'leverages', 'llms', 'to', 'generate', 'variable', 'queries', 'from', 'the', 'initial', 'query', 'using', 'customized', 'prompts', 'then', 'clusters', 'them', 'into', 'groups', 'to', 'distinctly', 'represent', 'diverse', 'intents', 'furthermore', 'the', 'framework', 'explores', 'to', 'combine', 'diverse', 'intents', 'query', 'with', 'innovative', 'weighted', 'aggregation', 'strategies', 'to', 'optimize', 'retrieval', 'performance', 'and', 'crucially', 'integrates', 'a', 'novel', 'query', 'evaluation', 'rewarding', 'model', 'qerm', 'to', 'refine', 'the', 'process', 'through', 'feedback', 'loops', 'empirical', 'experiments', 'on', 'the', 'beir', 'benchmark', 'demonstrate', 'that', 'gencrf', 'achieves', 'state-of-the-art', 'performance', 'surpassing', 'previous', 'query', 'reformulation', 'sotas', 'by', 'up', 'to', '12%', 'on', 'ndcg@10', 'these', 'techniques', 'can', 'be', 'adapted', 'to', 'various', 'llms', 'significantly', 'boosting', 'retriever', 'performance', 'and', 'advancing', 'the', 'field', 'of', 'information', 'retrieval'], 'D5': ['session', 'search', 'involves', 'a', 'series', 'of', 'interactive', 'queries', 'and', 'actions', 'to', 'fulfill', 'user', 's', 'complex', 'information', 'need', 'current', 'strategies', 'typically', 'prioritize', 'sequential', 'modeling', 'for', 'deep', 'semantic', 'understanding', 'overlooking', 'the', 'graph', 'structure', 'in', 'interactions', 'while', 'some', 'approaches', 'focus', 'on', 'capturing', 'structural', 'information', 'they', 'use', 'a', 'generalized', 'representation', 'for', 'documents', 'neglecting', 'the', 'word-level', 'semantic', 'modeling', 'in', 'this', 'paper', 'we', 'propose', 'symbolic', 'graph', 'ranker', 'sgr', 'which', 'aims', 'to', 'take', 'advantage', 'of', 'both', 'text-based', 'and', 'graph-based', 'approaches', 'by', 'leveraging', 'the', 'power', 'of', 'recent', 'large', 'language', 'models', 'llms', 'concretely', 'we', 'first', 'introduce', 'a', 'set', 'of', 'symbolic', 'grammar', 'rules', 'to', 'convert', 'session', 'graph', 'into', 'text', 'this', 'allows', 'integrating', 'session', 'history', 'interaction', 'process', 'and', 'task', 'instruction', 'seamlessly', 'as', 'inputs', 'for', 'the', 'llm', 'moreover', 'given', 'the', 'natural', 'discrepancy', 'between', 'llms', 'pre-trained', 'on', 'textual', 'corpora', 'and', 'the', 'symbolic', 'language', 'we', 'produce', 'using', 'our', 'graph-to-text', 'grammar', 'our', 'objective', 'is', 'to', 'enhance', 'llms', 'ability', 'to', 'capture', 'graph', 'structures', 'within', 'a', 'textual', 'format', 'to', 'achieve', 'this', 'we', 'introduce', 'a', 'set', 'of', 'self-supervised', 'symbolic', 'learning', 'tasks', 'including', 'link', 'prediction', 'node', 'content', 'generation', 'and', 'generative', 'contrastive', 'learning', 'to', 'enable', 'llms', 'to', 'capture', 'the', 'topological', 'information', 'from', 'coarse-grained', 'to', 'fine-grained', 'experiment', 'results', 'and', 'comprehensive', 'analysis', 'on', 'two', 'benchmark', 'datasets', 'aol', 'and', 'tiangong-st', 'confirm', 'the', 'superiority', 'of', 'our', 'approach', 'our', 'paradigm', 'also', 'offers', 'a', 'novel', 'and', 'effective', 'methodology', 'that', 'bridges', 'the', 'gap', 'between', 'traditional', 'search', 'strategies', 'and', 'modern', 'llms'], 'D6': ['the', 'description', 'of', 'an', 'item', 'plays', 'a', 'pivotal', 'role', 'in', 'providing', 'concise', 'and', 'informative', 'summaries', 'to', 'captivate', 'potential', 'viewers', 'and', 'is', 'essential', 'for', 'recommendation', 'systems', 'traditionally', 'such', 'descriptions', 'were', 'obtained', 'through', 'manual', 'web', 'scraping', 'techniques', 'which', 'are', 'time-consuming', 'and', 'susceptible', 'to', 'data', 'inconsistencies', 'in', 'recent', 'years', 'large', 'language', 'models', 'llms', 'such', 'as', 'gpt-3.5', 'and', 'open', 'source', 'llms', 'like', 'alpaca', 'have', 'emerged', 'as', 'powerful', 'tools', 'for', 'natural', 'language', 'processing', 'tasks', 'in', 'this', 'paper', 'we', 'have', 'explored', 'how', 'we', 'can', 'use', 'llms', 'to', 'generate', 'detailed', 'descriptions', 'of', 'the', 'items', 'to', 'conduct', 'the', 'study', 'we', 'have', 'used', 'the', 'movielens', '1m', 'dataset', 'comprising', 'movie', 'titles', 'and', 'the', 'goodreads', 'dataset', 'consisting', 'of', 'names', 'of', 'books', 'and', 'subsequently', 'an', 'open-sourced', 'llm', 'alpaca', 'was', 'prompted', 'with', 'few-shot', 'prompting', 'on', 'this', 'dataset', 'to', 'generate', 'detailed', 'movie', 'descriptions', 'considering', 'multiple', 'features', 'like', 'the', 'names', 'of', 'the', 'cast', 'and', 'directors', 'for', 'the', 'ml', 'dataset', 'and', 'the', 'names', 'of', 'the', 'author', 'and', 'publisher', 'for', 'the', 'goodreads', 'dataset', 'the', 'generated', 'description', 'was', 'then', 'compared', 'with', 'the', 'scraped', 'descriptions', 'using', 'a', 'combination', 'of', 'top', 'hits', 'mrr', 'and', 'ndcg', 'as', 'evaluation', 'metrics', 'the', 'results', 'demonstrated', 'that', 'llm-based', 'movie', 'description', 'generation', 'exhibits', 'significant', 'promise', 'with', 'results', 'comparable', 'to', 'the', 'ones', 'obtained', 'by', 'web-scraped', 'descriptions']}\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "ExpReg = RegexpTokenizer('\\d+(?:-\\d+)*(?:\\.\\d+)*%|\\w+@\\d+|\\w+(?:\\/\\w+)*(?:\\.\\w+\\.)*(?:-\\w+(?:\\.\\d+)*)*')\n",
    "# Trec-dl-2019 2020\n",
    "# ExpReg = RegexpTokenizer('trec-dl\\s2019\\sand\\s2020|\\d+(?:-\\d+)*(?:\\.\\d+)*%|\\w+@\\d+|\\w+(?:\\/\\w+)*(?:\\.\\w+\\.)*(?:-\\w+(?:\\.\\d+)*)*')\n",
    "#(QR)\n",
    "# ExpReg = RegexpTokenizer('trec-dl\\s2019\\sand\\s2020|\\d+(?:\\.\\d+)*%|\\w+@\\d+|\\w+\\(\\w+\\)*|\\w+(?:\\/\\w+)*(?:\\.\\w+\\.)*(?:-\\w+(?:\\.\\d+)*)*')\n",
    "# Parentheses\n",
    "# ExpReg = RegexpTokenizer('trec-dl\\s2019\\sand\\s2020|\\d+(?:\\.\\d+)*%|\\w+@\\d+|\\w+\\(\\w+\\)*|\\(\\w+\\)|\\w+(?:\\/\\w+)*(?:\\.\\w+\\.)*(?:-\\w+(?:\\.\\d+)*)*')\n",
    "# MovieLens 1M\n",
    "# ExpReg = RegexpTokenizer('trec-dl\\s2019\\sand\\s2020|movielens\\s1m|\\d+(?:\\.\\d+)*%|\\w+@\\d+|\\w+\\(\\w+\\)*|\\(\\w+\\)|\\w+(?:\\/\\w+)*(?:\\.\\w+\\.)*(?:-\\w+(?:\\.\\d+)*)*')\n",
    "termesRegex = {file_name: ExpReg.tokenize(txt.lower()) for file_name, txt in txt_contents.items()} \n",
    "print(termesRegex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D1': ['query', 'reformulation', 'qr', 'set', 'techniques', 'used', 'transform', 'user', 'original', 'search', 'query', 'text', 'better', 'aligns', 'user', 'intent', 'improves', 'search', 'experience', 'recently', 'zero-shot', 'qr', 'shown', 'promising', 'approach', 'due', 'ability', 'exploit', 'knowledge', 'inherent', 'large', 'language', 'models', 'taking', 'inspiration', 'success', 'ensemble', 'prompting', 'strategies', 'benefited', 'many', 'tasks', 'investigate', 'help', 'improve', 'query', 'reformulation', 'context', 'propose', 'ensemble', 'based', 'prompting', 'technique', 'genqrensemble', 'leverages', 'paraphrases', 'zero-shot', 'instruction', 'generate', 'multiple', 'sets', 'keywords', 'ultimately', 'improving', 'retrieval', 'performance', 'introduce', 'post-retrieval', 'variant', 'genqrensemblerf', 'incorporate', 'pseudo', 'relevant', 'feedback', 'evaluations', 'four', 'ir', 'benchmarks', 'find', 'genqrensemble', 'generates', 'better', 'reformulations', 'relative', 'ndcg@10', 'improvements', '18%', 'map', 'improvements', 'upto', '24%', 'previous', 'zero-shot', 'state-of-art', 'msmarco', 'passage', 'ranking', 'task', 'genqrensemblerf', 'shows', 'relative', 'gains', '5%', 'mrr', 'using', 'pseudo-relevance', 'feedback', '9%', 'ndcg@10', 'using', 'relevant', 'feedback', 'documents'], 'D2': ['ranking', 'documents', 'using', 'large', 'language', 'models', 'llms', 'directly', 'feeding', 'query', 'candidate', 'documents', 'prompt', 'interesting', 'practical', 'problem', 'however', 'researchers', 'found', 'difficult', 'outperform', 'fine-tuned', 'baseline', 'rankers', 'benchmark', 'datasets', 'analyze', 'pointwise', 'listwise', 'ranking', 'prompts', 'used', 'existing', 'methods', 'argue', 'off-the-shelf', 'llms', 'fully', 'understand', 'challenging', 'ranking', 'formulations', 'paper', 'propose', 'significantly', 'reduce', 'burden', 'llms', 'using', 'new', 'technique', 'called', 'pairwise', 'ranking', 'prompting', 'prp', 'results', 'first', 'literature', 'achieve', 'state-of-the-art', 'ranking', 'performance', 'standard', 'benchmarks', 'using', 'moderate-sized', 'open-sourced', 'llms', 'trec-dl', '2019', '2020', 'prp', 'based', 'flan-ul2', 'model', '20b', 'parameters', 'performs', 'favorably', 'previous', 'best', 'approach', 'literature', 'based', 'blackbox', 'commercial', 'gpt-4', '50x', 'estimated', 'model', 'size', 'outperforming', 'llm-based', 'solutions', 'instructgpt', '175b', 'parameters', '12-10%', 'ranking', 'metrics', 'using', 'prompt', 'template', 'seven', 'beir', 'tasks', 'prp', 'outperforms', 'supervised', 'baselines', 'outperforms', 'blackbox', 'commercial', 'chatgpt', 'solution', '4.2%', 'pointwise', 'llm-based', 'solutions', '10%', 'average', 'ndcg@10', 'furthermore', 'propose', 'several', 'variants', 'prp', 'improve', 'efficiency', 'show', 'possible', 'achieve', 'competitive', 'results', 'even', 'linear', 'complexity'], 'D3': ['large', 'language', 'models', 'llm', 'manifested', 'unparalleled', 'modeling', 'capability', 'various', 'tasks', 'e.g.', 'multi-step', 'reasoning', 'input', 'models', 'mostly', 'limited', 'plain', 'text', 'could', 'long', 'contain', 'noisy', 'information', 'long', 'text', 'could', 'take', 'long', 'time', 'process', 'thus', 'may', 'efficient', 'enough', 'recommender', 'systems', 'require', 'immediate', 'response', 'llm-based', 'recommendation', 'models', 'user', 'item', 'ids', 'usually', 'filled', 'template', 'i.e.', 'discrete', 'prompt', 'allow', 'models', 'understand', 'given', 'task', 'models', 'usually', 'need', 'extensive', 'fine-tuning', 'bridge', 'user/item', 'ids', 'template', 'words', 'unleash', 'power', 'llm', 'recommendation', 'address', 'problems', 'propose', 'distill', 'discrete', 'prompt', 'specific', 'task', 'set', 'continuous', 'prompt', 'vectors', 'bridge', 'ids', 'words', 'reduce', 'inference', 'time', 'also', 'design', 'training', 'strategy', 'attempt', 'improve', 'efficiency', 'training', 'models', 'experimental', 'results', 'three', 'real-world', 'datasets', 'demonstrate', 'effectiveness', 'prompt', 'distillation', 'pod', 'approach', 'sequential', 'recommendation', 'top-n', 'recommendation', 'tasks', 'although', 'training', 'efficiency', 'significantly', 'improved', 'improvement', 'inference', 'efficiency', 'limited', 'finding', 'may', 'inspire', 'researchers', 'community', 'improve', 'inference', 'efficiency', 'llm-based', 'recommendation', 'models'], 'D4': ['query', 'reformulation', 'well-known', 'problem', 'information', 'retrieval', 'ir', 'aimed', 'enhancing', 'single', 'search', 'successful', 'completion', 'rate', 'automatically', 'modifying', 'user', 'input', 'query', 'recent', 'methods', 'leverage', 'large', 'language', 'models', 'llms', 'improve', 'query', 'reformulation', 'often', 'generate', 'limited', 'redundant', 'expansions', 'potentially', 'constraining', 'effectiveness', 'capturing', 'diverse', 'intents', 'paper', 'propose', 'gencrf', 'generative', 'clustering', 'reformulation', 'framework', 'capture', 'diverse', 'intentions', 'adaptively', 'based', 'multiple', 'differentiated', 'well-generated', 'queries', 'retrieval', 'phase', 'first', 'time', 'gencrf', 'leverages', 'llms', 'generate', 'variable', 'queries', 'initial', 'query', 'using', 'customized', 'prompts', 'clusters', 'groups', 'distinctly', 'represent', 'diverse', 'intents', 'furthermore', 'framework', 'explores', 'combine', 'diverse', 'intents', 'query', 'innovative', 'weighted', 'aggregation', 'strategies', 'optimize', 'retrieval', 'performance', 'crucially', 'integrates', 'novel', 'query', 'evaluation', 'rewarding', 'model', 'qerm', 'refine', 'process', 'feedback', 'loops', 'empirical', 'experiments', 'beir', 'benchmark', 'demonstrate', 'gencrf', 'achieves', 'state-of-the-art', 'performance', 'surpassing', 'previous', 'query', 'reformulation', 'sotas', '12%', 'ndcg@10', 'techniques', 'adapted', 'various', 'llms', 'significantly', 'boosting', 'retriever', 'performance', 'advancing', 'field', 'information', 'retrieval'], 'D5': ['session', 'search', 'involves', 'series', 'interactive', 'queries', 'actions', 'fulfill', 'user', 'complex', 'information', 'need', 'current', 'strategies', 'typically', 'prioritize', 'sequential', 'modeling', 'deep', 'semantic', 'understanding', 'overlooking', 'graph', 'structure', 'interactions', 'approaches', 'focus', 'capturing', 'structural', 'information', 'use', 'generalized', 'representation', 'documents', 'neglecting', 'word-level', 'semantic', 'modeling', 'paper', 'propose', 'symbolic', 'graph', 'ranker', 'sgr', 'aims', 'take', 'advantage', 'text-based', 'graph-based', 'approaches', 'leveraging', 'power', 'recent', 'large', 'language', 'models', 'llms', 'concretely', 'first', 'introduce', 'set', 'symbolic', 'grammar', 'rules', 'convert', 'session', 'graph', 'text', 'allows', 'integrating', 'session', 'history', 'interaction', 'process', 'task', 'instruction', 'seamlessly', 'inputs', 'llm', 'moreover', 'given', 'natural', 'discrepancy', 'llms', 'pre-trained', 'textual', 'corpora', 'symbolic', 'language', 'produce', 'using', 'graph-to-text', 'grammar', 'objective', 'enhance', 'llms', 'ability', 'capture', 'graph', 'structures', 'within', 'textual', 'format', 'achieve', 'introduce', 'set', 'self-supervised', 'symbolic', 'learning', 'tasks', 'including', 'link', 'prediction', 'node', 'content', 'generation', 'generative', 'contrastive', 'learning', 'enable', 'llms', 'capture', 'topological', 'information', 'coarse-grained', 'fine-grained', 'experiment', 'results', 'comprehensive', 'analysis', 'two', 'benchmark', 'datasets', 'aol', 'tiangong-st', 'confirm', 'superiority', 'approach', 'paradigm', 'also', 'offers', 'novel', 'effective', 'methodology', 'bridges', 'gap', 'traditional', 'search', 'strategies', 'modern', 'llms'], 'D6': ['description', 'item', 'plays', 'pivotal', 'role', 'providing', 'concise', 'informative', 'summaries', 'captivate', 'potential', 'viewers', 'essential', 'recommendation', 'systems', 'traditionally', 'descriptions', 'obtained', 'manual', 'web', 'scraping', 'techniques', 'time-consuming', 'susceptible', 'data', 'inconsistencies', 'recent', 'years', 'large', 'language', 'models', 'llms', 'gpt-3.5', 'open', 'source', 'llms', 'like', 'alpaca', 'emerged', 'powerful', 'tools', 'natural', 'language', 'processing', 'tasks', 'paper', 'explored', 'use', 'llms', 'generate', 'detailed', 'descriptions', 'items', 'conduct', 'study', 'used', 'movielens', '1m', 'dataset', 'comprising', 'movie', 'titles', 'goodreads', 'dataset', 'consisting', 'names', 'books', 'subsequently', 'open-sourced', 'llm', 'alpaca', 'prompted', 'few-shot', 'prompting', 'dataset', 'generate', 'detailed', 'movie', 'descriptions', 'considering', 'multiple', 'features', 'like', 'names', 'cast', 'directors', 'ml', 'dataset', 'names', 'author', 'publisher', 'goodreads', 'dataset', 'generated', 'description', 'compared', 'scraped', 'descriptions', 'using', 'combination', 'top', 'hits', 'mrr', 'ndcg', 'evaluation', 'metrics', 'results', 'demonstrated', 'llm-based', 'movie', 'description', 'generation', 'exhibits', 'significant', 'promise', 'results', 'comparable', 'ones', 'obtained', 'web-scraped', 'descriptions']}\n"
     ]
    }
   ],
   "source": [
    "MotsVides = stopwords.words('english')\n",
    "TermesSansMotsVidesRegex = {file_name: [word for word in txt if word not in MotsVides] for file_name, txt in termesRegex.items()}\n",
    "print(TermesSansMotsVidesRegex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D1': ['queri', 'reformul', 'qr', 'set', 'techniqu', 'use', 'transform', 'user', 'origin', 'search', 'queri', 'text', 'better', 'align', 'user', 'intent', 'improv', 'search', 'experi', 'recent', 'zero-shot', 'qr', 'shown', 'promis', 'approach', 'due', 'abil', 'exploit', 'knowledg', 'inher', 'larg', 'languag', 'model', 'take', 'inspir', 'success', 'ensembl', 'prompt', 'strategi', 'benefit', 'mani', 'task', 'investig', 'help', 'improv', 'queri', 'reformul', 'context', 'propos', 'ensembl', 'base', 'prompt', 'techniqu', 'genqrensembl', 'leverag', 'paraphras', 'zero-shot', 'instruct', 'gener', 'multipl', 'set', 'keyword', 'ultim', 'improv', 'retriev', 'perform', 'introduc', 'post-retriev', 'variant', 'genqrensemblerf', 'incorpor', 'pseudo', 'relev', 'feedback', 'evalu', 'four', 'ir', 'benchmark', 'find', 'genqrensembl', 'gener', 'better', 'reformul', 'rel', 'ndcg@10', 'improv', '18%', 'map', 'improv', 'upto', '24%', 'previou', 'zero-shot', 'state-of-art', 'msmarco', 'passag', 'rank', 'task', 'genqrensemblerf', 'show', 'rel', 'gain', '5%', 'mrr', 'use', 'pseudo-relev', 'feedback', '9%', 'ndcg@10', 'use', 'relev', 'feedback', 'document'], 'D2': ['rank', 'document', 'use', 'larg', 'languag', 'model', 'llm', 'directli', 'feed', 'queri', 'candid', 'document', 'prompt', 'interest', 'practic', 'problem', 'howev', 'research', 'found', 'difficult', 'outperform', 'fine-tun', 'baselin', 'ranker', 'benchmark', 'dataset', 'analyz', 'pointwis', 'listwis', 'rank', 'prompt', 'use', 'exist', 'method', 'argu', 'off-the-shelf', 'llm', 'fulli', 'understand', 'challeng', 'rank', 'formul', 'paper', 'propos', 'significantli', 'reduc', 'burden', 'llm', 'use', 'new', 'techniqu', 'call', 'pairwis', 'rank', 'prompt', 'prp', 'result', 'first', 'literatur', 'achiev', 'state-of-the-art', 'rank', 'perform', 'standard', 'benchmark', 'use', 'moderate-s', 'open-sourc', 'llm', 'trec-dl', '2019', '2020', 'prp', 'base', 'flan-ul2', 'model', '20b', 'paramet', 'perform', 'favor', 'previou', 'best', 'approach', 'literatur', 'base', 'blackbox', 'commerci', 'gpt-4', '50x', 'estim', 'model', 'size', 'outperform', 'llm-base', 'solut', 'instructgpt', '175b', 'paramet', '12-10%', 'rank', 'metric', 'use', 'prompt', 'templat', 'seven', 'beir', 'task', 'prp', 'outperform', 'supervis', 'baselin', 'outperform', 'blackbox', 'commerci', 'chatgpt', 'solut', '4.2%', 'pointwis', 'llm-base', 'solut', '10%', 'averag', 'ndcg@10', 'furthermor', 'propos', 'sever', 'variant', 'prp', 'improv', 'effici', 'show', 'possibl', 'achiev', 'competit', 'result', 'even', 'linear', 'complex'], 'D3': ['larg', 'languag', 'model', 'llm', 'manifest', 'unparallel', 'model', 'capabl', 'variou', 'task', 'e.g.', 'multi-step', 'reason', 'input', 'model', 'mostli', 'limit', 'plain', 'text', 'could', 'long', 'contain', 'noisi', 'inform', 'long', 'text', 'could', 'take', 'long', 'time', 'process', 'thu', 'may', 'effici', 'enough', 'recommend', 'system', 'requir', 'immedi', 'respons', 'llm-base', 'recommend', 'model', 'user', 'item', 'id', 'usual', 'fill', 'templat', 'i.e.', 'discret', 'prompt', 'allow', 'model', 'understand', 'given', 'task', 'model', 'usual', 'need', 'extens', 'fine-tun', 'bridg', 'user/item', 'id', 'templat', 'word', 'unleash', 'power', 'llm', 'recommend', 'address', 'problem', 'propos', 'distil', 'discret', 'prompt', 'specif', 'task', 'set', 'continu', 'prompt', 'vector', 'bridg', 'id', 'word', 'reduc', 'infer', 'time', 'also', 'design', 'train', 'strategi', 'attempt', 'improv', 'effici', 'train', 'model', 'experiment', 'result', 'three', 'real-world', 'dataset', 'demonstr', 'effect', 'prompt', 'distil', 'pod', 'approach', 'sequenti', 'recommend', 'top-n', 'recommend', 'task', 'although', 'train', 'effici', 'significantli', 'improv', 'improv', 'infer', 'effici', 'limit', 'find', 'may', 'inspir', 'research', 'commun', 'improv', 'infer', 'effici', 'llm-base', 'recommend', 'model'], 'D4': ['queri', 'reformul', 'well-known', 'problem', 'inform', 'retriev', 'ir', 'aim', 'enhanc', 'singl', 'search', 'success', 'complet', 'rate', 'automat', 'modifi', 'user', 'input', 'queri', 'recent', 'method', 'leverag', 'larg', 'languag', 'model', 'llm', 'improv', 'queri', 'reformul', 'often', 'gener', 'limit', 'redund', 'expans', 'potenti', 'constrain', 'effect', 'captur', 'divers', 'intent', 'paper', 'propos', 'gencrf', 'gener', 'cluster', 'reformul', 'framework', 'captur', 'divers', 'intent', 'adapt', 'base', 'multipl', 'differenti', 'well-gener', 'queri', 'retriev', 'phase', 'first', 'time', 'gencrf', 'leverag', 'llm', 'gener', 'variabl', 'queri', 'initi', 'queri', 'use', 'custom', 'prompt', 'cluster', 'group', 'distinctli', 'repres', 'divers', 'intent', 'furthermor', 'framework', 'explor', 'combin', 'divers', 'intent', 'queri', 'innov', 'weight', 'aggreg', 'strategi', 'optim', 'retriev', 'perform', 'crucial', 'integr', 'novel', 'queri', 'evalu', 'reward', 'model', 'qerm', 'refin', 'process', 'feedback', 'loop', 'empir', 'experi', 'beir', 'benchmark', 'demonstr', 'gencrf', 'achiev', 'state-of-the-art', 'perform', 'surpass', 'previou', 'queri', 'reformul', 'sota', '12%', 'ndcg@10', 'techniqu', 'adapt', 'variou', 'llm', 'significantli', 'boost', 'retriev', 'perform', 'advanc', 'field', 'inform', 'retriev'], 'D5': ['session', 'search', 'involv', 'seri', 'interact', 'queri', 'action', 'fulfil', 'user', 'complex', 'inform', 'need', 'current', 'strategi', 'typic', 'priorit', 'sequenti', 'model', 'deep', 'semant', 'understand', 'overlook', 'graph', 'structur', 'interact', 'approach', 'focu', 'captur', 'structur', 'inform', 'use', 'gener', 'represent', 'document', 'neglect', 'word-level', 'semant', 'model', 'paper', 'propos', 'symbol', 'graph', 'ranker', 'sgr', 'aim', 'take', 'advantag', 'text-bas', 'graph-bas', 'approach', 'leverag', 'power', 'recent', 'larg', 'languag', 'model', 'llm', 'concret', 'first', 'introduc', 'set', 'symbol', 'grammar', 'rule', 'convert', 'session', 'graph', 'text', 'allow', 'integr', 'session', 'histori', 'interact', 'process', 'task', 'instruct', 'seamlessli', 'input', 'llm', 'moreov', 'given', 'natur', 'discrep', 'llm', 'pre-train', 'textual', 'corpora', 'symbol', 'languag', 'produc', 'use', 'graph-to-text', 'grammar', 'object', 'enhanc', 'llm', 'abil', 'captur', 'graph', 'structur', 'within', 'textual', 'format', 'achiev', 'introduc', 'set', 'self-supervis', 'symbol', 'learn', 'task', 'includ', 'link', 'predict', 'node', 'content', 'gener', 'gener', 'contrast', 'learn', 'enabl', 'llm', 'captur', 'topolog', 'inform', 'coarse-grain', 'fine-grain', 'experi', 'result', 'comprehens', 'analysi', 'two', 'benchmark', 'dataset', 'aol', 'tiangong-st', 'confirm', 'superior', 'approach', 'paradigm', 'also', 'offer', 'novel', 'effect', 'methodolog', 'bridg', 'gap', 'tradit', 'search', 'strategi', 'modern', 'llm'], 'D6': ['descript', 'item', 'play', 'pivot', 'role', 'provid', 'concis', 'inform', 'summari', 'captiv', 'potenti', 'viewer', 'essenti', 'recommend', 'system', 'tradit', 'descript', 'obtain', 'manual', 'web', 'scrape', 'techniqu', 'time-consum', 'suscept', 'data', 'inconsist', 'recent', 'year', 'larg', 'languag', 'model', 'llm', 'gpt-3.5', 'open', 'sourc', 'llm', 'like', 'alpaca', 'emerg', 'power', 'tool', 'natur', 'languag', 'process', 'task', 'paper', 'explor', 'use', 'llm', 'gener', 'detail', 'descript', 'item', 'conduct', 'studi', 'use', 'movielen', '1m', 'dataset', 'compris', 'movi', 'titl', 'goodread', 'dataset', 'consist', 'name', 'book', 'subsequ', 'open-sourc', 'llm', 'alpaca', 'prompt', 'few-shot', 'prompt', 'dataset', 'gener', 'detail', 'movi', 'descript', 'consid', 'multipl', 'featur', 'like', 'name', 'cast', 'director', 'ml', 'dataset', 'name', 'author', 'publish', 'goodread', 'dataset', 'gener', 'descript', 'compar', 'scrape', 'descript', 'use', 'combin', 'top', 'hit', 'mrr', 'ndcg', 'evalu', 'metric', 'result', 'demonstr', 'llm-base', 'movi', 'descript', 'gener', 'exhibit', 'signific', 'promis', 'result', 'compar', 'one', 'obtain', 'web-scrap', 'descript']}\n",
      "{'D1': ['query', 'reform', 'qr', 'set', 'techn', 'us', 'transform', 'us', 'origin', 'search', 'query', 'text', 'bet', 'align', 'us', 'int', 'improv', 'search', 'expery', 'rec', 'zero-shot', 'qr', 'shown', 'prom', 'approach', 'due', 'abl', 'exploit', 'knowledg', 'inh', 'larg', 'langu', 'model', 'tak', 'inspir', 'success', 'ensembl', 'prompt', 'strategies', 'benefit', 'many', 'task', 'investig', 'help', 'improv', 'query', 'reform', 'context', 'propos', 'ensembl', 'bas', 'prompt', 'techn', 'genqrensembl', 'lev', 'paraphras', 'zero-shot', 'instruct', 'gen', 'multipl', 'set', 'keyword', 'ultim', 'improv', 'retriev', 'perform', 'introduc', 'post-retrieval', 'vary', 'genqrensemblerf', 'incorp', 'pseudo', 'relev', 'feedback', 'evalu', 'four', 'ir', 'benchmark', 'find', 'genqrensembl', 'gen', 'bet', 'reform', 'rel', 'ndcg@10', 'improv', '18%', 'map', 'improv', 'upto', '24%', 'prevy', 'zero-shot', 'state-of-art', 'msmarco', 'pass', 'rank', 'task', 'genqrensemblerf', 'show', 'rel', 'gain', '5%', 'mrr', 'us', 'pseudo-relevance', 'feedback', '9%', 'ndcg@10', 'us', 'relev', 'feedback', 'docu'], 'D2': ['rank', 'docu', 'us', 'larg', 'langu', 'model', 'llms', 'direct', 'fee', 'query', 'candid', 'docu', 'prompt', 'interest', 'pract', 'problem', 'howev', 'research', 'found', 'difficult', 'outperform', 'fine-tuned', 'baselin', 'rank', 'benchmark', 'dataset', 'analys', 'pointw', 'listw', 'rank', 'prompt', 'us', 'ex', 'method', 'argu', 'off-the-shelf', 'llms', 'ful', 'understand', 'challeng', 'rank', 'form', 'pap', 'propos', 'sign', 'reduc', 'burd', 'llms', 'us', 'new', 'techn', 'cal', 'pairw', 'rank', 'prompt', 'prp', 'result', 'first', 'lit', 'achiev', 'state-of-the-art', 'rank', 'perform', 'standard', 'benchmark', 'us', 'moderate-sized', 'open-sourced', 'llms', 'trec-dl', '2019', '2020', 'prp', 'bas', 'flan-ul2', 'model', '20b', 'paramet', 'perform', 'fav', 'prevy', 'best', 'approach', 'lit', 'bas', 'blackbox', 'commerc', 'gpt-4', '50x', 'estim', 'model', 'siz', 'outperform', 'llm-based', 'solv', 'instructgpt', '175b', 'paramet', '12-10%', 'rank', 'met', 'us', 'prompt', 'templ', 'sev', 'beir', 'task', 'prp', 'outperform', 'superv', 'baselin', 'outperform', 'blackbox', 'commerc', 'chatgpt', 'solv', '4.2%', 'pointw', 'llm-based', 'solv', '10%', 'av', 'ndcg@10', 'furtherm', 'propos', 'sev', 'vary', 'prp', 'improv', 'efficy', 'show', 'poss', 'achiev', 'competit', 'result', 'ev', 'linear', 'complex'], 'D3': ['larg', 'langu', 'model', 'llm', 'manifest', 'unparallel', 'model', 'cap', 'vary', 'task', 'e.g.', 'multi-step', 'reason', 'input', 'model', 'most', 'limit', 'plain', 'text', 'could', 'long', 'contain', 'noisy', 'inform', 'long', 'text', 'could', 'tak', 'long', 'tim', 'process', 'thu', 'may', 'efficy', 'enough', 'recommend', 'system', 'requir', 'immedy', 'respons', 'llm-based', 'recommend', 'model', 'us', 'item', 'id', 'us', 'fil', 'templ', 'i.e.', 'discret', 'prompt', 'allow', 'model', 'understand', 'giv', 'task', 'model', 'us', 'nee', 'extend', 'fine-tuning', 'bridg', 'user/item', 'id', 'templ', 'word', 'unleash', 'pow', 'llm', 'recommend', 'address', 'problem', 'propos', 'distil', 'discret', 'prompt', 'spec', 'task', 'set', 'continu', 'prompt', 'vect', 'bridg', 'id', 'word', 'reduc', 'inf', 'tim', 'also', 'design', 'train', 'strategy', 'attempt', 'improv', 'efficy', 'train', 'model', 'expery', 'result', 'three', 'real-world', 'dataset', 'demonst', 'effect', 'prompt', 'distil', 'pod', 'approach', 'sequ', 'recommend', 'top-n', 'recommend', 'task', 'although', 'train', 'efficy', 'sign', 'improv', 'improv', 'inf', 'efficy', 'limit', 'find', 'may', 'inspir', 'research', 'commun', 'improv', 'inf', 'efficy', 'llm-based', 'recommend', 'model'], 'D4': ['query', 'reform', 'well-known', 'problem', 'inform', 'retriev', 'ir', 'aim', 'enh', 'singl', 'search', 'success', 'complet', 'rat', 'autom', 'mod', 'us', 'input', 'query', 'rec', 'method', 'lev', 'larg', 'langu', 'model', 'llms', 'improv', 'query', 'reform', 'oft', 'gen', 'limit', 'redund', 'expand', 'pot', 'constrain', 'effect', 'capt', 'divers', 'int', 'pap', 'propos', 'gencrf', 'gen', 'clust', 'reform', 'framework', 'capt', 'divers', 'int', 'adapt', 'bas', 'multipl', 'differenty', 'well-generated', 'query', 'retriev', 'phas', 'first', 'tim', 'gencrf', 'lev', 'llms', 'gen', 'vary', 'query', 'init', 'query', 'us', 'custom', 'prompt', 'clust', 'group', 'distinct', 'repres', 'divers', 'int', 'furtherm', 'framework', 'expl', 'combin', 'divers', 'int', 'query', 'innov', 'weight', 'aggreg', 'strategies', 'optim', 'retriev', 'perform', 'cruc', 'integr', 'novel', 'query', 'evalu', 'reward', 'model', 'qerm', 'refin', 'process', 'feedback', 'loop', 'empir', 'expery', 'beir', 'benchmark', 'demonst', 'gencrf', 'achiev', 'state-of-the-art', 'perform', 'surpass', 'prevy', 'query', 'reform', 'sota', '12%', 'ndcg@10', 'techn', 'adapt', 'vary', 'llms', 'sign', 'boost', 'retriev', 'perform', 'adv', 'field', 'inform', 'retriev'], 'D5': ['sess', 'search', 'involv', 'sery', 'interact', 'query', 'act', 'fulfil', 'us', 'complex', 'inform', 'nee', 'cur', 'strategies', 'typ', 'priorit', 'sequ', 'model', 'deep', 'sem', 'understand', 'overlook', 'graph', 'structure', 'interact', 'approach', 'foc', 'capt', 'structural', 'inform', 'us', 'gen', 'repres', 'docu', 'neglect', 'word-level', 'sem', 'model', 'pap', 'propos', 'symbol', 'graph', 'rank', 'sgr', 'aim', 'tak', 'adv', 'text-based', 'graph-based', 'approach', 'lev', 'pow', 'rec', 'larg', 'langu', 'model', 'llms', 'concret', 'first', 'introduc', 'set', 'symbol', 'gramm', 'rul', 'convert', 'sess', 'graph', 'text', 'allow', 'integr', 'sess', 'hist', 'interact', 'process', 'task', 'instruct', 'seamless', 'input', 'llm', 'moreov', 'giv', 'nat', 'discrep', 'llms', 'pre-trained', 'text', 'corpor', 'symbol', 'langu', 'produc', 'us', 'graph-to-text', 'gramm', 'object', 'enh', 'llms', 'abl', 'capt', 'graph', 'structures', 'within', 'text', 'form', 'achiev', 'introduc', 'set', 'self-supervised', 'symbol', 'learn', 'task', 'includ', 'link', 'predict', 'nod', 'cont', 'gen', 'gen', 'contrast', 'learn', 'en', 'llms', 'capt', 'topolog', 'inform', 'coarse-grained', 'fine-grained', 'expery', 'result', 'comprehend', 'analys', 'two', 'benchmark', 'dataset', 'aol', 'tiangong-st', 'confirm', 'supery', 'approach', 'paradigm', 'also', 'off', 'novel', 'effect', 'methodolog', 'bridg', 'gap', 'tradit', 'search', 'strategies', 'modern', 'llms'], 'D6': ['describ', 'item', 'play', 'pivot', 'rol', 'provid', 'cont', 'inform', 'sum', 'capt', 'pot', 'view', 'ess', 'recommend', 'system', 'tradit', 'describ', 'obtain', 'man', 'web', 'scraping', 'techn', 'time-consuming', 'suscept', 'dat', 'inconsist', 'rec', 'year', 'larg', 'langu', 'model', 'llms', 'gpt-3.5', 'op', 'sourc', 'llms', 'lik', 'alpac', 'emerg', 'pow', 'tool', 'nat', 'langu', 'process', 'task', 'pap', 'expl', 'us', 'llms', 'gen', 'detail', 'describ', 'item', 'conduc', 'study', 'us', 'moviel', '1m', 'dataset', 'compr', 'movy', 'titl', 'goodread', 'dataset', 'consist', 'nam', 'book', 'subsequ', 'open-sourced', 'llm', 'alpac', 'prompt', 'few-shot', 'prompt', 'dataset', 'gen', 'detail', 'movy', 'describ', 'consid', 'multipl', 'feat', 'lik', 'nam', 'cast', 'direct', 'ml', 'dataset', 'nam', 'auth', 'publ', 'goodread', 'dataset', 'gen', 'describ', 'comp', 'scraped', 'describ', 'us', 'combin', 'top', 'hit', 'mrr', 'ndcg', 'evalu', 'met', 'result', 'demonst', 'llm-based', 'movy', 'describ', 'gen', 'exhibit', 'sign', 'prom', 'result', 'comp', 'on', 'obtain', 'web-scraped', 'describ']}\n"
     ]
    }
   ],
   "source": [
    "TermesNormalisationPorterRegex = {file_name: [Porter.stem(word) for word in txt] for file_name, txt in TermesSansMotsVidesRegex.items()}\n",
    "print(TermesNormalisationPorterRegex)\n",
    "TermesNormalisationLancasterRegex = {file_name: [Lancaster.stem(word) for word in txt] for file_name, txt in TermesSansMotsVidesRegex.items()}\n",
    "print(TermesNormalisationLancasterRegex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. 18%', '1. 24%', '1. 5%', '1. 9%', '1. abil', '1. align', '1. approach', '1. base', '1. benchmark', '1. benefit', '1. better', '1. context', '1. document', '1. due', '1. ensembl', '1. evalu', '1. experi', '1. exploit', '1. feedback', '1. find', '1. four', '1. gain', '1. gener', '1. genqrensembl', '1. genqrensemblerf', '1. help', '1. improv', '1. incorpor', '1. inher', '1. inspir', '1. instruct', '1. intent', '1. introduc', '1. investig', '1. ir', '1. keyword', '1. knowledg', '1. languag', '1. larg', '1. leverag', '1. mani', '1. map', '1. model', '1. mrr', '1. msmarco', '1. multipl', '1. ndcg@10', '1. origin', '1. paraphras', '1. passag', '1. perform', '1. post-retriev', '1. previou', '1. promis', '1. prompt', '1. propos', '1. pseudo', '1. pseudo-relev', '1. qr', '1. queri', '1. rank', '1. recent', '1. reformul', '1. rel', '1. relev', '1. retriev', '1. search', '1. set', '1. show', '1. shown', '1. state-of-art', '1. strategi', '1. success', '1. take', '1. task', '1. techniqu', '1. text', '1. transform', '1. ultim', '1. upto', '1. use', '1. user', '1. variant', '1. zero-shot', '2. 10%', '2. 12-10%', '2. 175b', '2. 2019', '2. 2020', '2. 20b', '2. 4.2%', '2. 50x', '2. achiev', '2. analyz', '2. approach', '2. argu', '2. averag', '2. base', '2. baselin', '2. beir', '2. benchmark', '2. best', '2. blackbox', '2. burden', '2. call', '2. candid', '2. challeng', '2. chatgpt', '2. commerci', '2. competit', '2. complex', '2. dataset', '2. difficult', '2. directli', '2. document', '2. effici', '2. estim', '2. even', '2. exist', '2. favor', '2. feed', '2. fine-tun', '2. first', '2. flan-ul2', '2. formul', '2. found', '2. fulli', '2. furthermor', '2. gpt-4', '2. howev', '2. improv', '2. instructgpt', '2. interest', '2. languag', '2. larg', '2. linear', '2. listwis', '2. literatur', '2. llm', '2. llm-base', '2. method', '2. metric', '2. model', '2. moderate-s', '2. ndcg@10', '2. new', '2. off-the-shelf', '2. open-sourc', '2. outperform', '2. pairwis', '2. paper', '2. paramet', '2. perform', '2. pointwis', '2. possibl', '2. practic', '2. previou', '2. problem', '2. prompt', '2. propos', '2. prp', '2. queri', '2. rank', '2. ranker', '2. reduc', '2. research', '2. result', '2. seven', '2. sever', '2. show', '2. significantli', '2. size', '2. solut', '2. standard', '2. state-of-the-art', '2. supervis', '2. task', '2. techniqu', '2. templat', '2. trec-dl', '2. understand', '2. use', '2. variant', '3. address', '3. allow', '3. also', '3. although', '3. approach', '3. attempt', '3. bridg', '3. capabl', '3. commun', '3. contain', '3. continu', '3. could', '3. dataset', '3. demonstr', '3. design', '3. discret', '3. distil', '3. e.g.', '3. effect', '3. effici', '3. enough', '3. experiment', '3. extens', '3. fill', '3. find', '3. fine-tun', '3. given', '3. i.e.', '3. id', '3. immedi', '3. improv', '3. infer', '3. inform', '3. input', '3. inspir', '3. item', '3. languag', '3. larg', '3. limit', '3. llm', '3. llm-base', '3. long', '3. manifest', '3. may', '3. model', '3. mostli', '3. multi-step', '3. need', '3. noisi', '3. plain', '3. pod', '3. power', '3. problem', '3. process', '3. prompt', '3. propos', '3. real-world', '3. reason', '3. recommend', '3. reduc', '3. requir', '3. research', '3. respons', '3. result', '3. sequenti', '3. set', '3. significantli', '3. specif', '3. strategi', '3. system', '3. take', '3. task', '3. templat', '3. text', '3. three', '3. thu', '3. time', '3. top-n', '3. train', '3. understand', '3. unleash', '3. unparallel', '3. user', '3. user/item', '3. usual', '3. variou', '3. vector', '3. word', '4. 12%', '4. achiev', '4. adapt', '4. advanc', '4. aggreg', '4. aim', '4. automat', '4. base', '4. beir', '4. benchmark', '4. boost', '4. captur', '4. cluster', '4. combin', '4. complet', '4. constrain', '4. crucial', '4. custom', '4. demonstr', '4. differenti', '4. distinctli', '4. divers', '4. effect', '4. empir', '4. enhanc', '4. evalu', '4. expans', '4. experi', '4. explor', '4. feedback', '4. field', '4. first', '4. framework', '4. furthermor', '4. gencrf', '4. gener', '4. group', '4. improv', '4. inform', '4. initi', '4. innov', '4. input', '4. integr', '4. intent', '4. ir', '4. languag', '4. larg', '4. leverag', '4. limit', '4. llm', '4. loop', '4. method', '4. model', '4. modifi', '4. multipl', '4. ndcg@10', '4. novel', '4. often', '4. optim', '4. paper', '4. perform', '4. phase', '4. potenti', '4. previou', '4. problem', '4. process', '4. prompt', '4. propos', '4. qerm', '4. queri', '4. rate', '4. recent', '4. redund', '4. refin', '4. reformul', '4. repres', '4. retriev', '4. reward', '4. search', '4. significantli', '4. singl', '4. sota', '4. state-of-the-art', '4. strategi', '4. success', '4. surpass', '4. techniqu', '4. time', '4. use', '4. user', '4. variabl', '4. variou', '4. weight', '4. well-gener', '4. well-known', '5. abil', '5. achiev', '5. action', '5. advantag', '5. aim', '5. allow', '5. also', '5. analysi', '5. aol', '5. approach', '5. benchmark', '5. bridg', '5. captur', '5. coarse-grain', '5. complex', '5. comprehens', '5. concret', '5. confirm', '5. content', '5. contrast', '5. convert', '5. corpora', '5. current', '5. dataset', '5. deep', '5. discrep', '5. document', '5. effect', '5. enabl', '5. enhanc', '5. experi', '5. fine-grain', '5. first', '5. focu', '5. format', '5. fulfil', '5. gap', '5. gener', '5. given', '5. grammar', '5. graph', '5. graph-bas', '5. graph-to-text', '5. histori', '5. includ', '5. inform', '5. input', '5. instruct', '5. integr', '5. interact', '5. introduc', '5. involv', '5. languag', '5. larg', '5. learn', '5. leverag', '5. link', '5. llm', '5. methodolog', '5. model', '5. modern', '5. moreov', '5. natur', '5. need', '5. neglect', '5. node', '5. novel', '5. object', '5. offer', '5. overlook', '5. paper', '5. paradigm', '5. power', '5. pre-train', '5. predict', '5. priorit', '5. process', '5. produc', '5. propos', '5. queri', '5. ranker', '5. recent', '5. represent', '5. result', '5. rule', '5. seamlessli', '5. search', '5. self-supervis', '5. semant', '5. sequenti', '5. seri', '5. session', '5. set', '5. sgr', '5. strategi', '5. structur', '5. superior', '5. symbol', '5. take', '5. task', '5. text', '5. text-bas', '5. textual', '5. tiangong-st', '5. topolog', '5. tradit', '5. two', '5. typic', '5. understand', '5. use', '5. user', '5. within', '5. word-level', '6. 1m', '6. alpaca', '6. author', '6. book', '6. captiv', '6. cast', '6. combin', '6. compar', '6. compris', '6. concis', '6. conduct', '6. consid', '6. consist', '6. data', '6. dataset', '6. demonstr', '6. descript', '6. detail', '6. director', '6. emerg', '6. essenti', '6. evalu', '6. exhibit', '6. explor', '6. featur', '6. few-shot', '6. gener', '6. goodread', '6. gpt-3.5', '6. hit', '6. inconsist', '6. inform', '6. item', '6. languag', '6. larg', '6. like', '6. llm', '6. llm-base', '6. manual', '6. metric', '6. ml', '6. model', '6. movi', '6. movielen', '6. mrr', '6. multipl', '6. name', '6. natur', '6. ndcg', '6. obtain', '6. one', '6. open', '6. open-sourc', '6. paper', '6. pivot', '6. play', '6. potenti', '6. power', '6. process', '6. promis', '6. prompt', '6. provid', '6. publish', '6. recent', '6. recommend', '6. result', '6. role', '6. scrape', '6. signific', '6. sourc', '6. studi', '6. subsequ', '6. summari', '6. suscept', '6. system', '6. task', '6. techniqu', '6. time-consum', '6. titl', '6. tool', '6. top', '6. tradit', '6. use', '6. viewer', '6. web', '6. web-scrap', '6. year']\n",
      "['1. 18%', '1. 24%', '1. 5%', '1. 9%', '1. abil', '1. align', '1. approach', '1. base', '1. benchmark', '1. benefit', '1. better', '1. context', '1. document', '1. due', '1. ensembl', '1. evalu', '1. experi', '1. exploit', '1. feedback', '1. find', '1. four', '1. gain', '1. gener', '1. genqrensembl', '1. genqrensemblerf', '1. help', '1. improv', '1. incorpor', '1. inher', '1. inspir', '1. instruct', '1. intent', '1. introduc', '1. investig', '1. ir', '1. keyword', '1. knowledg', '1. languag', '1. larg', '1. leverag', '1. mani', '1. map', '1. model', '1. mrr', '1. msmarco', '1. multipl', '1. ndcg@10', '1. origin', '1. paraphras', '1. passag', '1. perform', '1. post-retriev', '1. previou', '1. promis', '1. prompt', '1. propos', '1. pseudo', '1. pseudo-relev', '1. qr', '1. queri', '1. rank', '1. recent', '1. reformul', '1. rel', '1. relev', '1. retriev', '1. search', '1. set', '1. show', '1. shown', '1. state-of-art', '1. strategi', '1. success', '1. take', '1. task', '1. techniqu', '1. text', '1. transform', '1. ultim', '1. upto', '1. use', '1. user', '1. variant', '1. zero-shot', '2. 10%', '2. 12-10%', '2. 175b', '2. 2019', '2. 2020', '2. 20b', '2. 4.2%', '2. 50x', '2. achiev', '2. analyz', '2. approach', '2. argu', '2. averag', '2. base', '2. baselin', '2. beir', '2. benchmark', '2. best', '2. blackbox', '2. burden', '2. call', '2. candid', '2. challeng', '2. chatgpt', '2. commerci', '2. competit', '2. complex', '2. dataset', '2. difficult', '2. directli', '2. document', '2. effici', '2. estim', '2. even', '2. exist', '2. favor', '2. feed', '2. fine-tun', '2. first', '2. flan-ul2', '2. formul', '2. found', '2. fulli', '2. furthermor', '2. gpt-4', '2. howev', '2. improv', '2. instructgpt', '2. interest', '2. languag', '2. larg', '2. linear', '2. listwis', '2. literatur', '2. llm', '2. llm-base', '2. method', '2. metric', '2. model', '2. moderate-s', '2. ndcg@10', '2. new', '2. off-the-shelf', '2. open-sourc', '2. outperform', '2. pairwis', '2. paper', '2. paramet', '2. perform', '2. pointwis', '2. possibl', '2. practic', '2. previou', '2. problem', '2. prompt', '2. propos', '2. prp', '2. queri', '2. rank', '2. ranker', '2. reduc', '2. research', '2. result', '2. seven', '2. sever', '2. show', '2. significantli', '2. size', '2. solut', '2. standard', '2. state-of-the-art', '2. supervis', '2. task', '2. techniqu', '2. templat', '2. trec-dl', '2. understand', '2. use', '2. variant', '3. address', '3. allow', '3. also', '3. although', '3. approach', '3. attempt', '3. bridg', '3. capabl', '3. commun', '3. contain', '3. continu', '3. could', '3. dataset', '3. demonstr', '3. design', '3. discret', '3. distil', '3. e.g.', '3. effect', '3. effici', '3. enough', '3. experiment', '3. extens', '3. fill', '3. find', '3. fine-tun', '3. given', '3. i.e.', '3. id', '3. immedi', '3. improv', '3. infer', '3. inform', '3. input', '3. inspir', '3. item', '3. languag', '3. larg', '3. limit', '3. llm', '3. llm-base', '3. long', '3. manifest', '3. may', '3. model', '3. mostli', '3. multi-step', '3. need', '3. noisi', '3. plain', '3. pod', '3. power', '3. problem', '3. process', '3. prompt', '3. propos', '3. real-world', '3. reason', '3. recommend', '3. reduc', '3. requir', '3. research', '3. respons', '3. result', '3. sequenti', '3. set', '3. significantli', '3. specif', '3. strategi', '3. system', '3. take', '3. task', '3. templat', '3. text', '3. three', '3. thu', '3. time', '3. top-n', '3. train', '3. understand', '3. unleash', '3. unparallel', '3. user', '3. user/item', '3. usual', '3. variou', '3. vector', '3. word', '4. 12%', '4. achiev', '4. adapt', '4. advanc', '4. aggreg', '4. aim', '4. automat', '4. base', '4. beir', '4. benchmark', '4. boost', '4. captur', '4. cluster', '4. combin', '4. complet', '4. constrain', '4. crucial', '4. custom', '4. demonstr', '4. differenti', '4. distinctli', '4. divers', '4. effect', '4. empir', '4. enhanc', '4. evalu', '4. expans', '4. experi', '4. explor', '4. feedback', '4. field', '4. first', '4. framework', '4. furthermor', '4. gencrf', '4. gener', '4. group', '4. improv', '4. inform', '4. initi', '4. innov', '4. input', '4. integr', '4. intent', '4. ir', '4. languag', '4. larg', '4. leverag', '4. limit', '4. llm', '4. loop', '4. method', '4. model', '4. modifi', '4. multipl', '4. ndcg@10', '4. novel', '4. often', '4. optim', '4. paper', '4. perform', '4. phase', '4. potenti', '4. previou', '4. problem', '4. process', '4. prompt', '4. propos', '4. qerm', '4. queri', '4. rate', '4. recent', '4. redund', '4. refin', '4. reformul', '4. repres', '4. retriev', '4. reward', '4. search', '4. significantli', '4. singl', '4. sota', '4. state-of-the-art', '4. strategi', '4. success', '4. surpass', '4. techniqu', '4. time', '4. use', '4. user', '4. variabl', '4. variou', '4. weight', '4. well-gener', '4. well-known', '5. abil', '5. achiev', '5. action', '5. advantag', '5. aim', '5. allow', '5. also', '5. analysi', '5. aol', '5. approach', '5. benchmark', '5. bridg', '5. captur', '5. coarse-grain', '5. complex', '5. comprehens', '5. concret', '5. confirm', '5. content', '5. contrast', '5. convert', '5. corpora', '5. current', '5. dataset', '5. deep', '5. discrep', '5. document', '5. effect', '5. enabl', '5. enhanc', '5. experi', '5. fine-grain', '5. first', '5. focu', '5. format', '5. fulfil', '5. gap', '5. gener', '5. given', '5. grammar', '5. graph', '5. graph-bas', '5. graph-to-text', '5. histori', '5. includ', '5. inform', '5. input', '5. instruct', '5. integr', '5. interact', '5. introduc', '5. involv', '5. languag', '5. larg', '5. learn', '5. leverag', '5. link', '5. llm', '5. methodolog', '5. model', '5. modern', '5. moreov', '5. natur', '5. need', '5. neglect', '5. node', '5. novel', '5. object', '5. offer', '5. overlook', '5. paper', '5. paradigm', '5. power', '5. pre-train', '5. predict', '5. priorit', '5. process', '5. produc', '5. propos', '5. queri', '5. ranker', '5. recent', '5. represent', '5. result', '5. rule', '5. seamlessli', '5. search', '5. self-supervis', '5. semant', '5. sequenti', '5. seri', '5. session', '5. set', '5. sgr', '5. strategi', '5. structur', '5. superior', '5. symbol', '5. take', '5. task', '5. text', '5. text-bas', '5. textual', '5. tiangong-st', '5. topolog', '5. tradit', '5. two', '5. typic', '5. understand', '5. use', '5. user', '5. within', '5. word-level', '6. 1m', '6. alpaca', '6. author', '6. book', '6. captiv', '6. cast', '6. combin', '6. compar', '6. compris', '6. concis', '6. conduct', '6. consid', '6. consist', '6. data', '6. dataset', '6. demonstr', '6. descript', '6. detail', '6. director', '6. emerg', '6. essenti', '6. evalu', '6. exhibit', '6. explor', '6. featur', '6. few-shot', '6. gener', '6. goodread', '6. gpt-3.5', '6. hit', '6. inconsist', '6. inform', '6. item', '6. languag', '6. larg', '6. like', '6. llm', '6. llm-base', '6. manual', '6. metric', '6. ml', '6. model', '6. movi', '6. movielen', '6. mrr', '6. multipl', '6. name', '6. natur', '6. ndcg', '6. obtain', '6. one', '6. open', '6. open-sourc', '6. paper', '6. pivot', '6. play', '6. potenti', '6. power', '6. process', '6. promis', '6. prompt', '6. provid', '6. publish', '6. recent', '6. recommend', '6. result', '6. role', '6. scrape', '6. signific', '6. sourc', '6. studi', '6. subsequ', '6. summari', '6. suscept', '6. system', '6. task', '6. techniqu', '6. time-consum', '6. titl', '6. tool', '6. top', '6. tradit', '6. use', '6. viewer', '6. web', '6. web-scrap', '6. year']\n"
     ]
    }
   ],
   "source": [
    "TermesSortedPorterRegex = {file_name: sorted(txt) for file_name, txt in TermesNormalisationPorterRegex.items()}\n",
    "\n",
    "all_termesPorterRegex = []\n",
    "for idx, (file_name, termesRegex) in enumerate(TermesSortedPorterRegex.items(), start=1):\n",
    "    unique_termesPorterRegex = set(termesRegex)\n",
    "    sorted_termesPorterRegex = sorted(unique_termesPorterRegex)\n",
    "    final_termesPorterRegex = [f'{idx}. {terme}' for terme in sorted_termesPorterRegex]\n",
    "    all_termesPorterRegex += final_termesPorterRegex\n",
    "\n",
    "all_termesPorterRegex = sorted(all_termesPorterRegex)\n",
    "\n",
    "print(all_termesPorterRegex)\n",
    "\n",
    "\n",
    "\n",
    "TermesSortedLancasterRegex = {file_name: sorted(txt) for file_name, txt in TermesNormalisationLancasterRegex.items()}\n",
    "\n",
    "all_termesLancasterRegex = []\n",
    "for idx, (file_name, termesRegex) in enumerate(TermesSortedPorterRegex.items(), start=1):\n",
    "    unique_termesLancasterRegex = set(termesRegex)\n",
    "    sorted_termesLancasterRegex = sorted(unique_termesLancasterRegex)\n",
    "    final_termesLancasterRegex = [f'{idx}. {terme}' for terme in sorted_termesLancasterRegex]\n",
    "    all_termesLancasterRegex += final_termesLancasterRegex\n",
    "\n",
    "all_termesLancasterRegex = sorted(all_termesLancasterRegex)\n",
    "\n",
    "\n",
    "\n",
    "print(all_termesLancasterRegex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=\"../Index/DescripteursTokenPorter.txt\", mode='w') as f :\n",
    "    for terme in all_termesPorterRegex:\n",
    "            f.write(f'{terme}\\n')\n",
    "\n",
    "with open(file=\"../Index/DescripteursTokenLancaster.txt\", mode='w') as f :\n",
    "    for terme in all_termesLancasterRegex:\n",
    "            f.write(f'{terme}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_termesPorterRegex = []\n",
    "for idx, (file_name, termesRegex) in enumerate(TermesSortedPorterRegex.items(), start=1):\n",
    "    unique_termesPorterRegex = set(termesRegex)\n",
    "    sorted_termesPorterRegex = sorted(unique_termesPorterRegex)\n",
    "    final_termesPorterRegex = [f'{terme} {idx}' for terme in sorted_termesPorterRegex]\n",
    "    all_termesPorterRegex += final_termesPorterRegex\n",
    "\n",
    "all_termesPorterRegex = sorted(all_termesPorterRegex)\n",
    "\n",
    "all_termesLancasterRegex = []\n",
    "for idx, (file_name, termesRegex) in enumerate(TermesSortedPorterRegex.items(), start=1):\n",
    "    unique_termesLancasterRegex = set(termesRegex)\n",
    "    sorted_termesLancasterRegex = sorted(unique_termesLancasterRegex)\n",
    "    final_termesLancasterRegex = [f'{terme} {idx}' for terme in sorted_termesLancasterRegex]\n",
    "    all_termesLancasterRegex += final_termesLancasterRegex\n",
    "\n",
    "all_termesLancasterRegex = sorted(all_termesLancasterRegex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=\"../Index/InverseTokenPorter.txt\", mode='w') as f :\n",
    "    for terme in all_termesPorterRegex:\n",
    "            f.write(f'{terme}\\n')\n",
    "\n",
    "with open(file=\"../Index/InverseTokenLancaster.txt\", mode='w') as f :\n",
    "    for terme in all_termesLancasterRegex:\n",
    "            f.write(f'{terme}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequences and Poids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def freq(t, d):\n",
    "    frequence = 0\n",
    "    for terme in d:\n",
    "        if t == terme:\n",
    "            frequence += 1\n",
    "    return frequence\n",
    "\n",
    "def maxfreq(d):\n",
    "    max = 0\n",
    "    for terme in d:\n",
    "        frq = freq(terme, d)\n",
    "        if frq > max:\n",
    "            max = frq\n",
    "    return max\n",
    "\n",
    "def ni(trm, collection):\n",
    "    ni = 0\n",
    "    for file_name, termes in collection.items(): \n",
    "            if trm in termes:\n",
    "                ni += 1\n",
    "    return ni\n",
    "\n",
    "\n",
    "def poids(t,d, N, ni, maxfreq, freq):\n",
    "    lg = math.log10((N/ni)+1)\n",
    "    fq = freq / maxfreq\n",
    "    return fq*lg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "TermesSortedSplit = {file_name: sorted(txt) for file_name, txt in TermesSansMotsVides.items()}\n",
    "\n",
    "all_termesFrequenceSplit = []\n",
    "for idx, (file_name, termesSplit) in enumerate(TermesSortedSplit.items(), start=1):\n",
    "    termesFrequencesSplit = [f'{idx}. {terme} {freq(terme, termesSplit)} {poids(t=terme, d=termesSplit, N=6, ni=ni(trm=terme, collection=TermesSortedSplit), maxfreq=maxfreq(termesSplit), freq=freq(terme, termesSplit))}' for terme in termesSplit]\n",
    "    unique_termesSplit = set(termesFrequencesSplit)\n",
    "    final_termesSplit = sorted(unique_termesSplit)\n",
    "    all_termesFrequenceSplit += final_termesSplit\n",
    "\n",
    "TermesSortedSplitPorter = {file_name: sorted(txt) for file_name, txt in TermesNormalisationPorter.items()}\n",
    "\n",
    "all_termesFrequenceSplitPorter = []\n",
    "for idx, (file_name, termesSplit) in enumerate(TermesSortedSplitPorter.items(), start=1):\n",
    "    termesFrequencesSplit = [f'{idx}. {terme} {freq(terme, termesSplit)} {poids(t=terme, d=termesSplit, N=6, ni=ni(trm=terme, collection=TermesSortedSplitPorter), maxfreq=maxfreq(termesSplit), freq=freq(terme, termesSplit))}' for terme in termesSplit]\n",
    "    unique_termesSplit = set(termesFrequencesSplit)\n",
    "    final_termesSplit = sorted(unique_termesSplit)\n",
    "    all_termesFrequenceSplitPorter += final_termesSplit\n",
    "\n",
    "TermesSortedSplitLancaster = {file_name: sorted(txt) for file_name, txt in TermesNormalisationLancaster.items()}\n",
    "\n",
    "all_termesFrequenceSplitLancaster = []\n",
    "for idx, (file_name, termesSplit) in enumerate(TermesSortedSplitLancaster.items(), start=1):\n",
    "    termesFrequencesSplit = [f'{idx}. {terme} {freq(terme, termesSplit)} {poids(t=terme, d=termesSplit, N=6, ni=ni(trm=terme, collection=TermesSortedSplitLancaster), maxfreq=maxfreq(termesSplit), freq=freq(terme, termesSplit))}' for terme in termesSplit]\n",
    "    unique_termesSplit = set(termesFrequencesSplit)\n",
    "    final_termesSplit = sorted(unique_termesSplit)\n",
    "    all_termesFrequenceSplitLancaster += final_termesSplit\n",
    "\n",
    "# for element in all_termesFrequenceSplit:\n",
    "#     print(element)\n",
    "\n",
    "\n",
    "\n",
    "TermesSortedRegex = {file_name: sorted(txt) for file_name, txt in TermesSansMotsVidesRegex.items()}\n",
    "\n",
    "all_termesFrequenceRegex = []\n",
    "for idx, (file_name, termesRegex) in enumerate(TermesSortedRegex.items(), start=1):\n",
    "    termesFrequencesRegex = [f'{idx}. {terme} {freq(terme, termesRegex)} {poids(t=terme, d=termesRegex, N=6, ni=ni(trm=terme, collection=TermesSortedRegex), maxfreq=maxfreq(termesRegex), freq=freq(terme, termesRegex))}' for terme in termesRegex]\n",
    "    unique_termesRegex = set(termesFrequencesRegex)\n",
    "    final_termesRegex = sorted(unique_termesRegex)\n",
    "    all_termesFrequenceRegex += final_termesRegex\n",
    "\n",
    "TermesSortedRegexPorter = {file_name: sorted(txt) for file_name, txt in TermesNormalisationPorterRegex.items()}\n",
    "\n",
    "all_termesFrequenceRegexPorter = []\n",
    "for idx, (file_name, termesRegex) in enumerate(TermesSortedRegexPorter.items(), start=1):\n",
    "    termesFrequencesRegex = [f'{idx}. {terme} {freq(terme, termesRegex)} {poids(t=terme, d=termesRegex, N=6, ni=ni(trm=terme, collection=TermesSortedRegexPorter), maxfreq=maxfreq(termesRegex), freq=freq(terme, termesRegex))}' for terme in termesRegex]\n",
    "    unique_termesRegex = set(termesFrequencesRegex)\n",
    "    final_termesRegex = sorted(unique_termesRegex)\n",
    "    all_termesFrequenceRegexPorter += final_termesRegex\n",
    "\n",
    "TermesSortedRegexLancaster = {file_name: sorted(txt) for file_name, txt in TermesNormalisationLancasterRegex.items()}\n",
    "\n",
    "all_termesFrequenceRegexLancaster = []\n",
    "for idx, (file_name, termesRegex) in enumerate(TermesSortedRegexLancaster.items(), start=1):\n",
    "    termesFrequencesRegex = [f'{idx}. {terme} {freq(terme, termesRegex)} {poids(t=terme, d=termesRegex, N=6, ni=ni(trm=terme, collection=TermesSortedRegexLancaster), maxfreq=maxfreq(termesRegex), freq=freq(terme, termesRegex))}' for terme in termesRegex]\n",
    "    unique_termesRegex = set(termesFrequencesRegex)\n",
    "    final_termesRegex = sorted(unique_termesRegex)\n",
    "    all_termesFrequenceRegexLancaster += final_termesRegex\n",
    "\n",
    "# for element in all_termesFrequenceRegex:\n",
    "#     print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=\"../Index/DescripteursSplit.txt\", mode='w') as f :\n",
    "    for terme in all_termesFrequenceSplit:\n",
    "        f.write(f'{terme}\\n')\n",
    "\n",
    "with open(file=\"../Index/DescripteursSplitPorter.txt\", mode='w') as f :\n",
    "    for terme in all_termesFrequenceSplitPorter:\n",
    "        f.write(f'{terme}\\n')\n",
    "\n",
    "with open(file=\"../Index/DescripteursSplitLancaster.txt\", mode='w') as f :\n",
    "    for terme in all_termesFrequenceSplitLancaster:\n",
    "        f.write(f'{terme}\\n')\n",
    "\n",
    "with open(file=\"../Index/DescripteursRegex.txt\", mode='w') as f :\n",
    "    for terme in all_termesFrequenceRegex:\n",
    "        f.write(f'{terme}\\n')\n",
    "\n",
    "with open(file=\"../Index/DescripteursRegexPorter.txt\", mode='w') as f :\n",
    "    for terme in all_termesFrequenceRegexPorter:\n",
    "        f.write(f'{terme}\\n')\n",
    "\n",
    "with open(file=\"../Index/DescripteursRegexLancaster.txt\", mode='w') as f :\n",
    "    for terme in all_termesFrequenceRegexLancaster:\n",
    "        f.write(f'{terme}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "TermesSortedSplit = {file_name: sorted(txt) for file_name, txt in TermesSansMotsVides.items()}\n",
    "\n",
    "all_termesFrequenceSplit = []\n",
    "for idx, (file_name, termesSplit) in enumerate(TermesSortedSplit.items(), start=1):\n",
    "    termesFrequencesSplit = [f'{terme} {idx} {freq(terme, termesSplit)} {poids(t=terme, d=termesSplit, N=6, ni=ni(trm=terme, collection=TermesSortedSplit), maxfreq=maxfreq(termesSplit), freq=freq(terme, termesSplit))}' for terme in termesSplit]\n",
    "    unique_termesSplit = set(termesFrequencesSplit)\n",
    "    final_termesSplit = sorted(unique_termesSplit)\n",
    "    all_termesFrequenceSplit += final_termesSplit\n",
    "\n",
    "\n",
    "TermesSortedSplitPorter = {file_name: sorted(txt) for file_name, txt in TermesNormalisationPorter.items()}\n",
    "\n",
    "all_termesFrequenceSplitPorter = []\n",
    "for idx, (file_name, termesSplit) in enumerate(TermesSortedSplitPorter.items(), start=1):\n",
    "    termesFrequencesSplit = [f'{terme} {idx} {freq(terme, termesSplit)} {poids(t=terme, d=termesSplit, N=6, ni=ni(trm=terme, collection=TermesSortedSplitPorter), maxfreq=maxfreq(termesSplit), freq=freq(terme, termesSplit))}' for terme in termesSplit]\n",
    "    unique_termesSplit = set(termesFrequencesSplit)\n",
    "    final_termesSplit = sorted(unique_termesSplit)\n",
    "    all_termesFrequenceSplitPorter += final_termesSplit\n",
    "\n",
    "\n",
    "TermesSortedSplitLancaster = {file_name: sorted(txt) for file_name, txt in TermesNormalisationLancaster.items()}\n",
    "\n",
    "all_termesFrequenceSplitLancaster = []\n",
    "for idx, (file_name, termesSplit) in enumerate(TermesSortedSplitLancaster.items(), start=1):\n",
    "    termesFrequencesSplit = [f'{terme} {idx} {freq(terme, termesSplit)} {poids(t=terme, d=termesSplit, N=6, ni=ni(trm=terme, collection=TermesSortedSplitLancaster), maxfreq=maxfreq(termesSplit), freq=freq(terme, termesSplit))}' for terme in termesSplit]\n",
    "    unique_termesSplit = set(termesFrequencesSplit)\n",
    "    final_termesSplit = sorted(unique_termesSplit)\n",
    "    all_termesFrequenceSplitLancaster += final_termesSplit\n",
    "\n",
    "# for element in all_termesFrequenceSplit:\n",
    "#     print(element)\n",
    "\n",
    "\n",
    "\n",
    "TermesSortedRegex = {file_name: sorted(txt) for file_name, txt in TermesSansMotsVidesRegex.items()}\n",
    "\n",
    "all_termesFrequenceRegex = []\n",
    "for idx, (file_name, termesRegex) in enumerate(TermesSortedRegex.items(), start=1):\n",
    "    termesFrequencesRegex = [f'{terme} {idx} {freq(terme, termesRegex)} {poids(t=terme, d=termesRegex, N=6, ni=ni(trm=terme, collection=TermesSortedRegex), maxfreq=maxfreq(termesRegex), freq=freq(terme, termesRegex))}' for terme in termesRegex]\n",
    "    unique_termesRegex = set(termesFrequencesRegex)\n",
    "    final_termesRegex = sorted(unique_termesRegex)\n",
    "    all_termesFrequenceRegex += final_termesRegex\n",
    "\n",
    "TermesSortedRegexPorter = {file_name: sorted(txt) for file_name, txt in TermesNormalisationPorterRegex.items()}\n",
    "\n",
    "all_termesFrequenceRegexPorter = []\n",
    "for idx, (file_name, termesRegex) in enumerate(TermesSortedRegexPorter.items(), start=1):\n",
    "    termesFrequencesRegex = [f'{terme} {idx} {freq(terme, termesRegex)} {poids(t=terme, d=termesRegex, N=6, ni=ni(trm=terme, collection=TermesSortedRegexPorter), maxfreq=maxfreq(termesRegex), freq=freq(terme, termesRegex))}' for terme in termesRegex]\n",
    "    unique_termesRegex = set(termesFrequencesRegex)\n",
    "    final_termesRegex = sorted(unique_termesRegex)\n",
    "    all_termesFrequenceRegexPorter += final_termesRegex\n",
    "\n",
    "TermesSortedRegexLancaster = {file_name: sorted(txt) for file_name, txt in TermesNormalisationLancasterRegex.items()}\n",
    "\n",
    "all_termesFrequenceRegexLancaster = []\n",
    "for idx, (file_name, termesRegex) in enumerate(TermesSortedRegexLancaster.items(), start=1):\n",
    "    termesFrequencesRegex = [f'{terme} {idx} {freq(terme, termesRegex)} {poids(t=terme, d=termesRegex, N=6, ni=ni(trm=terme, collection=TermesSortedRegexLancaster), maxfreq=maxfreq(termesRegex), freq=freq(terme, termesRegex))}' for terme in termesRegex]\n",
    "    unique_termesRegex = set(termesFrequencesRegex)\n",
    "    final_termesRegex = sorted(unique_termesRegex)\n",
    "    all_termesFrequenceRegexLancaster += final_termesRegex\n",
    "\n",
    "# for element in all_termesFrequenceRegex:\n",
    "#     print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=\"../Index/InverseSplit.txt\", mode='w') as f :\n",
    "    for terme in all_termesFrequenceSplit:\n",
    "        f.write(f'{terme}\\n')\n",
    "\n",
    "with open(file=\"../Index/InverseSplitPorter.txt\", mode='w') as f :\n",
    "    for terme in all_termesFrequenceSplitPorter:\n",
    "        f.write(f'{terme}\\n')\n",
    "\n",
    "with open(file=\"../Index/InverseSplitLancaster.txt\", mode='w') as f :\n",
    "    for terme in all_termesFrequenceSplitLancaster:\n",
    "        f.write(f'{terme}\\n')\n",
    "\n",
    "with open(file=\"../Index/InverseRegex.txt\", mode='w') as f :\n",
    "    for terme in all_termesFrequenceRegex:\n",
    "        f.write(f'{terme}\\n')\n",
    "\n",
    "\n",
    "with open(file=\"../Index/InverseRegexPorter.txt\", mode='w') as f :\n",
    "    for terme in all_termesFrequenceRegexPorter:\n",
    "        f.write(f'{terme}\\n')\n",
    "\n",
    "with open(file=\"../Index/InverseRegexLancaster.txt\", mode='w') as f :\n",
    "    for terme in all_termesFrequenceRegexLancaster:\n",
    "        f.write(f'{terme}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "from PyQt5 import QtWidgets, uic\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QTableWidget, QTableWidgetItem \n",
    "import sys, os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "class MyWindow(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        uic.loadUi(\"../interface.ui\", self)\n",
    "        \n",
    "\n",
    "\n",
    "        self.set_combo_defaults()\n",
    "        self.radio_term_per_docs.setChecked(True)\n",
    "        \n",
    "        # connect signals to functions\n",
    "        self.selected_file = None\n",
    "        self.radio_docs_per_term.toggled.connect(self.on_radio_button_toggled)\n",
    "        self.combo_token.currentIndexChanged.connect(self.on_combo_changed)\n",
    "        self.combo_stemmer.currentIndexChanged.connect(self.on_stemmer_changed)\n",
    "        self.search_button.clicked.connect(self.show_results)\n",
    "    \n",
    "\n",
    "    def set_combo_defaults(self):\n",
    "        self.combo_token.setCurrentIndex(0)  # First item as default\n",
    "        self.combo_stemmer.setCurrentIndex(0)  # First item as default\n",
    "\n",
    "    def on_radio_button_toggled(self):\n",
    "        self.update_selected_file()\n",
    "    \n",
    "    def on_combo_changed(self):\n",
    "        self.update_selected_file()\n",
    "    \n",
    "    def on_stemmer_changed(self):\n",
    "        self.update_selected_file()\n",
    "\n",
    "    def update_selected_file(self):\n",
    "        token = self.combo_token.currentText()\n",
    "        stemmer = self.combo_stemmer.currentText()\n",
    "        query_type = \"Descripteurs\" if self.radio_term_per_docs.isChecked() else \"Inverse\"\n",
    "\n",
    "\n",
    "        if token == \"Split\":\n",
    "            file_suffix = \"Split\"\n",
    "        elif token == \"Regex\":\n",
    "            file_suffix = \"Regex\"\n",
    "\n",
    "        if stemmer == \"Porter\":\n",
    "            file_suffix += \"Porter\"\n",
    "        elif stemmer == \"Lancaster\":\n",
    "            file_suffix += \"Lancaster\"\n",
    "        elif stemmer == \"Without\":\n",
    "            file_suffix += \"\"\n",
    "        \n",
    "        self.selected_file = f\"{query_type}{file_suffix}.txt\"\n",
    "    \n",
    "\n",
    "    def show_results(self):\n",
    "        self.update_selected_file()\n",
    "        query = self.query_input.text()\n",
    "        self.display_file_content(query)\n",
    "\n",
    "    def display_file_content(self, query):\n",
    "        directory = \"../Index/\"\n",
    "        if not os.path.exists(f\"{directory}{self.selected_file}\"):\n",
    "            print(f\"path {directory}/{self.selected_file} doesn't exist\")\n",
    "            return\n",
    "        with open(f\"{directory}{self.selected_file}\", \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        doc_vocabulary = 0\n",
    "        total_frequency = 0\n",
    "    \n",
    "        \n",
    "        self.tableWidget.clearContents()\n",
    "        \n",
    "        self.tableWidget.setRowCount(0)\n",
    "        self.tableWidget.setColumnCount(4)\n",
    "\n",
    "        if self.radio_term_per_docs.isChecked():\n",
    "            self.tableWidget.setHorizontalHeaderLabels([\"Doc Num\", \"Term\", \"Freq\", \"Weight\"])\n",
    "        else:\n",
    "            self.tableWidget.setHorizontalHeaderLabels([\"Term\", \"Doc Num\", \"Freq\", \"Weight\"])\n",
    "\n",
    "        \n",
    "        query_doc_num = query.strip()  # Remove any extra whitespace\n",
    "        query_terme = query  # Remove any extra whitespace\n",
    "\n",
    "        row_count = -1 \n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) < 4:\n",
    "            \n",
    "                print(\"Line format incorrect:\", line)\n",
    "                continue                \n",
    "            \n",
    "            \n",
    "            \n",
    "            if self.radio_term_per_docs.isChecked():\n",
    "                doc_num = parts[0]\n",
    "                term = parts[1]\n",
    "                freq = int(parts[2])\n",
    "                weight = \"{:.4f}\".format(float(parts[3]))\n",
    "                \n",
    "                doc_num = doc_num.replace(\".\", \"\")\n",
    "                if doc_num != query_doc_num:  \n",
    "                    continue\n",
    "                \n",
    "                row_count += 1\n",
    "                self.tableWidget.insertRow(row_count)\n",
    "                self.tableWidget.setItem(row_count, 0, QTableWidgetItem(str(doc_num)))\n",
    "                self.tableWidget.setItem(row_count, 1, QTableWidgetItem(str(term)))\n",
    "                self.tableWidget.setItem(row_count, 2, QTableWidgetItem(str(freq)))\n",
    "                self.tableWidget.setItem(row_count, 3, QTableWidgetItem(str(weight)))\n",
    "\n",
    "                doc_vocabulary += 1\n",
    "                total_frequency += freq\n",
    "            \n",
    "            else:\n",
    "                term = parts[0]\n",
    "                doc_num = parts[1]\n",
    "                freq = int(parts[2])\n",
    "                weight = \"{:.4f}\".format(float(parts[3]))\n",
    "\n",
    "                if term != query_terme:  # Only show rows where doc_num matches query\n",
    "                    continue\n",
    "                \n",
    "                row_count += 1\n",
    "                self.tableWidget.insertRow(row_count)\n",
    "                self.tableWidget.setItem(row_count, 0, QTableWidgetItem(str(term)))\n",
    "                self.tableWidget.setItem(row_count, 1, QTableWidgetItem(str(doc_num)))\n",
    "                self.tableWidget.setItem(row_count, 2, QTableWidgetItem(str(freq)))\n",
    "                self.tableWidget.setItem(row_count, 3, QTableWidgetItem(str(weight)))\n",
    "\n",
    "                # self.add_table_row(row_num, [term, doc_num, freq, weight])\n",
    "            \n",
    "        if self.radio_term_per_docs.isChecked():\n",
    "            self.vocabulary_label.setText(f\"# doc vocabulary: {doc_vocabulary}\") \n",
    "            self.size_label.setText(f\"# doc size: {total_frequency}\")\n",
    "\n",
    "        if self.radio_docs_per_term.isChecked():\n",
    "            self.vocabulary_label.setText(\"\") \n",
    "            self.size_label.setText(\"\") \n",
    "            \n",
    "def window():\n",
    "    app = QApplication(sys.argv)\n",
    "    win = MyWindow() \n",
    "\n",
    "    win.show()\n",
    "    sys.exit(app.exec_())\n",
    "\n",
    "window()            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
